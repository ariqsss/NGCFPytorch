{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3214,
     "status": "ok",
     "timestamp": 1653631552178,
     "user": {
      "displayName": "Ariq Sudibyo",
      "userId": "10795734971894492673"
     },
     "user_tz": -420
    },
    "id": "l0gLLS0jFQtS",
    "outputId": "9f42b92d-85d3-4efb-a035-00fca2b25bd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from time import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.stats import truncnorm\n",
    "#from utils.load_data import Data\n",
    "#from utils.parser import parse_args\n",
    "#from utils.helper_functions import early_stopping,\\\n",
    "                                   #train,\\\n",
    "                                   #split_matrix,\\\n",
    "                                   #compute_ndcg_k,\\\n",
    "                                   #eval_model\n",
    "\n",
    "# read parsed arguments\n",
    "#args = parse_args()\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#data_dir = '/content/drive/My Drive/Colab Notebooks/NGCFPytorch/data/'\n",
    "data_dir = './data/'\n",
    "dataset = 'Gowella'\n",
    "batch_size = 1024\n",
    "layers = eval('[64,64]')\n",
    "emb_dim = 64\n",
    "lr = 0.0001\n",
    "reg = 1e-5\n",
    "mess_dropout = 0.1\n",
    "node_dropout = 0.\n",
    "k = 20\n",
    "argssave_results = 1\n",
    "argsn_epochs = 400\n",
    "argseval_N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "#torch. __version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZPqceEnh-ca"
   },
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mlGW8nb1FRpA"
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        # search train_file for max user_id/item_id\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    # first element is the user_id, rest are items\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    # item/user with highest number is number of items/users\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    # number of interactions\n",
    "                    self.n_train += len(items)\n",
    "\n",
    "        # search test_file for max item_id\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if not items:\n",
    "                        print(\"empyt test exists\")\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.n_items = max(self.n_items, max(items))\n",
    "                        self.n_test += len(items)\n",
    "        # adjust counters: user_id/item_id starts at 0\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        # create interactions/ratings matrix 'R' # dok = dictionary of keys\n",
    "        print('Creating interaction matrices R_train and R_test...')\n",
    "        t1 = time()\n",
    "        self.R_train = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32) \n",
    "        self.R_test = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        with open(train_file) as f_train:\n",
    "            with open(test_file) as f_test:\n",
    "                for l in f_train.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    items = [int(i) for i in l.split(' ')]\n",
    "                    uid, train_items = items[0], items[1:]\n",
    "                    # enter 1 if user interacted with item\n",
    "                    for i in train_items:\n",
    "                        self.R_train[uid, i] = 1.\n",
    "                    self.train_items[uid] = train_items\n",
    "\n",
    "                for l in f_test.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    uid, test_items = items[0], items[1:]\n",
    "                    for i in test_items:\n",
    "                        self.R_test[uid, i] = 1.0\n",
    "                    self.test_set[uid] = test_items\n",
    "        print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')\n",
    "\n",
    "    # if exist, get adjacency matrix\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
    "\n",
    "        except Exception:\n",
    "            print('Creating adjacency-matrix...')\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "        return adj_mat\n",
    "    \n",
    "    # create adjancency matrix\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        \n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R_train.tolil() # to list of lists\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        # normalize adjacency matrix\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -.5).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        print('Transforming adjacency-matrix to NGCF-adjacency matrix...')\n",
    "        ngcf_adj_mat = normalized_adj_single(adj_mat) + sp.eye(adj_mat.shape[0])\n",
    "\n",
    "        print('Complete. Transformed adjacency-matrix to NGCF-adjacency matrix in', time() - t2, 'sec.')\n",
    "        return ngcf_adj_mat.tocsr()\n",
    "\n",
    "    # create collections of N items that users never interacted with\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    # sample data for mini-batches\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-0RqzQFiA_l"
   },
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fHPt_8f5FRr_"
   },
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, flag_step, expected_order='asc'):\n",
    "    \"\"\"\n",
    "    Check if early_stopping is needed\n",
    "    Function copied from original code\n",
    "    \"\"\"\n",
    "    assert expected_order in ['asc', 'des']\n",
    "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'des' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= flag_step:\n",
    "        print(\"Early stopping at step: {} log:{}\".format(flag_step, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "\n",
    "    return best_value, stopping_step, should_stop\n",
    "\n",
    "def train(model, data_generator, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model PyTorch style\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    model: PyTorch model\n",
    "    data_generator: Data object\n",
    "    optimizer: PyTorch optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
    "    running_loss=0\n",
    "    for _ in range(n_batch):\n",
    "        u, i, j = data_generator.sample()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u,i,j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss\n",
    "\n",
    "def split_matrix(X, n_splits=100):\n",
    "    \"\"\"\n",
    "    Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    X: matrix to be split\n",
    "    n_folds: number of folds\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    splits: split matrices\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    pred_items: binary tensor with 1s in those locations corresponding to the predicted item interactions\n",
    "    test_items: binary tensor with 1s in locations corresponding to the real test interactions\n",
    "    test_indices: tensor with the location of the top-k predicted items\n",
    "    k: k'th-order \n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    NDCG@k\n",
    "    \"\"\"\n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
    "    dcg = (r[:, :k]/f).sum(1)\n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
    "    ndcg = dcg/dcg_max\n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def eval_model(u_emb, i_emb, Rtr, Rte, k):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    u_emb: User embeddings\n",
    "    i_emb: Item embeddings\n",
    "    Rtr: Sparse matrix with the training interactions\n",
    "    Rte: Sparse matrix with the testing interactions\n",
    "    k : kth-order for metrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result: Dictionary with lists correponding to the metrics at order k for k in Ks\n",
    "    \"\"\"\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        #pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
    "        pred_items.scatter_(dim=1, index=test_indices,src=torch.ones_like(test_indices).float().cuda())\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        #topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices[:, :k]).float().cuda())\n",
    "\n",
    "        TP = (test_items * topk_preds).sum(1)\n",
    "        rec = TP/test_items.sum(1)\n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(recall_k).mean(), torch.cat(ndcg_k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD1RU7SBiCe9"
   },
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JPHUDHUSDOz_"
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout,\n",
    "        adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.adj_mtx = adj_mtx\n",
    "        self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\n",
    "        self.reg = reg\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.node_dropout = node_dropout\n",
    "        self.mess_dropout = mess_dropout\n",
    "\n",
    "        self.u_g_embeddings = nn.Parameter(torch.empty(n_users, emb_dim+np.sum(self.layers)))\n",
    "        self.i_g_embeddings = nn.Parameter(torch.empty(n_items, emb_dim+np.sum(self.layers)))\n",
    "        \n",
    "        \n",
    "       # self.delta_P = tf.Variable(tf.zeros(shape=[self.num_users, self.embedding_size]),\n",
    "                          #         name='delta_P', dtype=tf.float32, trainable=False)  # (users, embedding_size)\n",
    "        #self.delta_P = nn.Parameter(torch.zeros([n_users, emb_dim+np.sum(self.layers)]),requires_grad=False)\n",
    "        self.delta_P = nn.Parameter(torch.zeros([batch_size, emb_dim+np.sum(self.layers)]).to(device),requires_grad=False)\n",
    "        \n",
    "       # self.delta_Qpos = tf.Variable(tf.zeros(shape=[self.num_items, self.embedding_size]),\n",
    "                           #            name='delta_Q', dtype=tf.float32, trainable=False)  # (items, embedding_size)\n",
    "        self.delta_Qpos = nn.Parameter(torch.zeros([batch_size,emb_dim+np.sum(self.layers)]).to(device),requires_grad=False)\n",
    "        #self.delta_Qneg = tf.Variable(tf.zeros(shape=[self.num_items, self.embedding_size]),\n",
    "         #                              name='delta_Q', dtype=tf.float32, trainable=False)  # (items, embedding_size)\n",
    "        self.delta_Qneg = nn.Parameter(torch.zeros([batch_size,emb_dim+np.sum(self.layers)]).to(device),requires_grad=False)\n",
    "\n",
    "        #self.h = tf.constant(1.0, tf.float32, [emb_dim, 1], name=\"h\")\n",
    "        #self.h = torch.tensor\n",
    "        self.h = torch.ones([emb_dim, 1], dtype=torch.float64, device=device)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "        # Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\n",
    "        self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\n",
    "        self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\n",
    "        \n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        weight_size_list = [self.emb_dim] + self.layers\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            weight_dict['W_gc_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_gc_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "            \n",
    "            weight_dict['W_bi_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_bi_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        \n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "    \n",
    "    def _create_adversarial(self):\n",
    "       \n",
    "        # generate the adversarial weights by random method\n",
    "        # generation\n",
    "        #self.adv_P = tf.truncated_normal(shape=[self.num_users, self.embedding_size], mean=0.0, stddev=0.01)\n",
    "        self.adv_P = truncnorm.rvs(-0.01,0.01,size=[n_users,emb_dim])\n",
    "        #self.adv_Q = tf.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0.0, stddev=0.01)\n",
    "        self.adv_Q = truncnorm.rvs(-0.01,0.01,size=[n_users,emb_dim])\n",
    "        # normalization and multiply epsilon\n",
    "        self.update_P = self.delta_P.assign(torch.nn.functional.normalize(self.adv_P, p=2, dim=1) * 0.5)\n",
    "        self.update_Q = self.delta_Q.assign(torch.nn.functional.normalize(self.adv_P, p=2, dim=1) * 0.5)\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\n",
    "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
    "\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        # forward pass for 'n' propagation layers\n",
    "        for k in range(self.n_layers):\n",
    "\n",
    "            # weighted sum messages of neighbours\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum weighted sum messages of neighbours\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbours\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\n",
    "            # transformed bi messages of neighbours\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation \n",
    "            ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
    "            # + message dropout\n",
    "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
    "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
    "\n",
    "            # normalize activation\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings.append(norm_embeddings)\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        self.adv_P = torch.from_numpy(truncnorm.rvs(-0.01,0.01,size=[batch_size,emb_dim+np.sum(self.layers)]))\n",
    "        #t = torch.from_numpy(a)\n",
    "        self.adv_Q = torch.from_numpy(truncnorm.rvs(-0.01,0.01,size=[batch_size,emb_dim+np.sum(self.layers)]))\n",
    "        #t = torch.from_numpy(a)\n",
    "        #self.update_P = self.delta_P.assign(torch.nn.functional.normalize(self.adv_P, p=2, dim=1) * 0.5)\n",
    "        self.delta_P = nn.Parameter(torch.nn.functional.normalize(self.adv_P, p=2, dim=1).to(device) * 0.5)\n",
    "        #self.update_Q = self.delta_Q.assign(torch.nn.functional.normalize(self.adv_P, p=2, dim=1) * 0.5)\n",
    "        self.delta_Qpos = nn.Parameter(torch.nn.functional.normalize(self.adv_Q, p=2, dim=1).to(device) * 0.5)\n",
    "        self.delta_Qneg = nn.Parameter(torch.nn.functional.normalize(self.adv_Q, p=2, dim=1).to(device) * 0.5)\n",
    "        \n",
    "        \n",
    "        # back to user/item dimension\n",
    "        u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\n",
    "        #u_g_embeddings = u_g_embeddings + self.delta_P\n",
    "        \n",
    "        self.u_g_embeddings = nn.Parameter(u_g_embeddings.float())\n",
    "        self.i_g_embeddings = nn.Parameter(i_g_embeddings)\n",
    "        #print(self.u_g_embeddings.size())\n",
    "        #print(self.delta_P.size())\n",
    "        \n",
    "        u_emb = u_g_embeddings[u] #+ torch.sum(self.delta_P, 1)# user embeddings\n",
    "        u_emb = u_emb + self.delta_P\n",
    "        \n",
    "        p_emb = i_g_embeddings[i] #+ torch.sum(torch.nn.Embedding(self.i_placeholder,emb_dim, _weight=self.delta_Qpos), 1) # positive item embeddings\n",
    "        p_emb = p_emb + self.delta_Qpos\n",
    "        \n",
    "        n_emb = i_g_embeddings[j]# + torch.sum(torch.nn.Embedding(self.i_placeholder,emb_dim, _weight=self.delta_Qneg), 1)# negative item embeddings\n",
    "        n_emb = n_emb + self.delta_Qneg\n",
    "        #print(u_emb.size())\n",
    "        #print(p_emb.size())\n",
    "        #print(n_emb)\n",
    "        y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\n",
    "        y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\n",
    "        log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
    "\n",
    "        # compute bpr-loss\n",
    "        bpr_loss = -log_prob\n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg  = self.reg*l2norm\n",
    "            bpr_loss =  -log_prob + l2reg\n",
    "\n",
    "        return bpr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0VfBH1ZiD7Q"
   },
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMy8xzwPESNw",
    "outputId": "8eafd7e9-09a3-471e-b2d8-64c81cc86677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n",
      "Creating interaction matrices R_train and R_test...\n",
      "Complete. Interaction matrices R_train and R_test created in 11.238682746887207 sec\n",
      "Loaded adjacency-matrix (shape: (70839, 70839) ) in 0.11396980285644531 sec.\n",
      "Initializing weights...\n",
      "Weights initialized.\n",
      "Start at 2022-05-29 22:51:05.010996\n",
      "Using cuda for computations\n",
      "Params on CUDA: True\n",
      "Epoch: 0, Training time: 213.58s, Loss: 533.9682\n",
      "Evaluate current model:\n",
      " Epoch: 0, Validation time: 38.51s \n",
      " Loss: 533.9682: \n",
      " Recall@20: 0.0018 \n",
      " NDCG@20: 0.0068\n",
      "Epoch: 1, Training time: 171.84s, Loss: 350.3316\n",
      "Evaluate current model:\n",
      " Epoch: 1, Validation time: 36.30s \n",
      " Loss: 350.3316: \n",
      " Recall@20: 0.0027 \n",
      " NDCG@20: 0.0091\n",
      "Epoch: 2, Training time: 170.17s, Loss: 301.6442\n",
      "Evaluate current model:\n",
      " Epoch: 2, Validation time: 36.57s \n",
      " Loss: 301.6442: \n",
      " Recall@20: 0.0033 \n",
      " NDCG@20: 0.0095\n",
      "Epoch: 3, Training time: 209.82s, Loss: 287.8875\n",
      "Evaluate current model:\n",
      " Epoch: 3, Validation time: 36.43s \n",
      " Loss: 287.8875: \n",
      " Recall@20: 0.0045 \n",
      " NDCG@20: 0.0128\n",
      "Epoch: 4, Training time: 171.08s, Loss: 278.0680\n",
      "Evaluate current model:\n",
      " Epoch: 4, Validation time: 36.39s \n",
      " Loss: 278.0680: \n",
      " Recall@20: 0.0065 \n",
      " NDCG@20: 0.0177\n",
      "Epoch: 5, Training time: 171.24s, Loss: 267.0240\n",
      "Evaluate current model:\n",
      " Epoch: 5, Validation time: 33.36s \n",
      " Loss: 267.0240: \n",
      " Recall@20: 0.0090 \n",
      " NDCG@20: 0.0216\n",
      "Epoch: 6, Training time: 172.57s, Loss: 254.8208\n",
      "Evaluate current model:\n",
      " Epoch: 6, Validation time: 32.69s \n",
      " Loss: 254.8208: \n",
      " Recall@20: 0.0166 \n",
      " NDCG@20: 0.0411\n",
      "Epoch: 7, Training time: 170.80s, Loss: 243.2401\n",
      "Evaluate current model:\n",
      " Epoch: 7, Validation time: 32.65s \n",
      " Loss: 243.2401: \n",
      " Recall@20: 0.0197 \n",
      " NDCG@20: 0.0440\n",
      "Epoch: 8, Training time: 170.76s, Loss: 233.3222\n",
      "Evaluate current model:\n",
      " Epoch: 8, Validation time: 32.57s \n",
      " Loss: 233.3222: \n",
      " Recall@20: 0.0255 \n",
      " NDCG@20: 0.0598\n",
      "Epoch: 9, Training time: 171.03s, Loss: 224.2453\n",
      "Evaluate current model:\n",
      " Epoch: 9, Validation time: 32.59s \n",
      " Loss: 224.2453: \n",
      " Recall@20: 0.0304 \n",
      " NDCG@20: 0.0652\n",
      "Epoch: 10, Training time: 170.84s, Loss: 215.9468\n",
      "Evaluate current model:\n",
      " Epoch: 10, Validation time: 32.91s \n",
      " Loss: 215.9468: \n",
      " Recall@20: 0.0329 \n",
      " NDCG@20: 0.0706\n",
      "Epoch: 11, Training time: 171.00s, Loss: 209.1736\n",
      "Evaluate current model:\n",
      " Epoch: 11, Validation time: 32.52s \n",
      " Loss: 209.1736: \n",
      " Recall@20: 0.0369 \n",
      " NDCG@20: 0.0832\n",
      "Epoch: 12, Training time: 172.58s, Loss: 202.5336\n",
      "Evaluate current model:\n",
      " Epoch: 12, Validation time: 35.51s \n",
      " Loss: 202.5336: \n",
      " Recall@20: 0.0404 \n",
      " NDCG@20: 0.0877\n",
      "Epoch: 13, Training time: 186.13s, Loss: 198.0223\n",
      "Evaluate current model:\n",
      " Epoch: 13, Validation time: 35.32s \n",
      " Loss: 198.0223: \n",
      " Recall@20: 0.0403 \n",
      " NDCG@20: 0.0882\n",
      "Epoch: 14, Training time: 185.30s, Loss: 193.3313\n",
      "Evaluate current model:\n",
      " Epoch: 14, Validation time: 35.80s \n",
      " Loss: 193.3313: \n",
      " Recall@20: 0.0417 \n",
      " NDCG@20: 0.0901\n",
      "Epoch: 15, Training time: 179.16s, Loss: 190.2357\n",
      "Evaluate current model:\n",
      " Epoch: 15, Validation time: 33.46s \n",
      " Loss: 190.2357: \n",
      " Recall@20: 0.0436 \n",
      " NDCG@20: 0.0956\n",
      "Epoch: 16, Training time: 171.84s, Loss: 187.3154\n",
      "Evaluate current model:\n",
      " Epoch: 16, Validation time: 33.10s \n",
      " Loss: 187.3154: \n",
      " Recall@20: 0.0461 \n",
      " NDCG@20: 0.1019\n",
      "Epoch: 17, Training time: 183.98s, Loss: 184.9298\n",
      "Evaluate current model:\n",
      " Epoch: 17, Validation time: 43.40s \n",
      " Loss: 184.9298: \n",
      " Recall@20: 0.0475 \n",
      " NDCG@20: 0.1043\n",
      "Epoch: 18, Training time: 180.22s, Loss: 182.9566\n",
      "Evaluate current model:\n",
      " Epoch: 18, Validation time: 34.13s \n",
      " Loss: 182.9566: \n",
      " Recall@20: 0.0498 \n",
      " NDCG@20: 0.1067\n",
      "Epoch: 19, Training time: 183.37s, Loss: 180.5260\n",
      "Evaluate current model:\n",
      " Epoch: 19, Validation time: 34.69s \n",
      " Loss: 180.5260: \n",
      " Recall@20: 0.0518 \n",
      " NDCG@20: 0.1137\n",
      "Epoch: 20, Training time: 177.97s, Loss: 178.9172\n",
      "Evaluate current model:\n",
      " Epoch: 20, Validation time: 34.23s \n",
      " Loss: 178.9172: \n",
      " Recall@20: 0.0527 \n",
      " NDCG@20: 0.1125\n",
      "Epoch: 21, Training time: 175.99s, Loss: 176.6448\n",
      "Evaluate current model:\n",
      " Epoch: 21, Validation time: 32.46s \n",
      " Loss: 176.6448: \n",
      " Recall@20: 0.0541 \n",
      " NDCG@20: 0.1163\n",
      "Epoch: 22, Training time: 170.62s, Loss: 174.8445\n",
      "Evaluate current model:\n",
      " Epoch: 22, Validation time: 32.58s \n",
      " Loss: 174.8445: \n",
      " Recall@20: 0.0551 \n",
      " NDCG@20: 0.1181\n",
      "Epoch: 23, Training time: 170.56s, Loss: 173.0931\n",
      "Evaluate current model:\n",
      " Epoch: 23, Validation time: 32.47s \n",
      " Loss: 173.0931: \n",
      " Recall@20: 0.0580 \n",
      " NDCG@20: 0.1258\n",
      "Epoch: 24, Training time: 170.33s, Loss: 171.3692\n",
      "Evaluate current model:\n",
      " Epoch: 24, Validation time: 32.35s \n",
      " Loss: 171.3692: \n",
      " Recall@20: 0.0576 \n",
      " NDCG@20: 0.1238\n",
      "Epoch: 25, Training time: 170.26s, Loss: 169.7057\n",
      "Evaluate current model:\n",
      " Epoch: 25, Validation time: 32.42s \n",
      " Loss: 169.7057: \n",
      " Recall@20: 0.0585 \n",
      " NDCG@20: 0.1248\n",
      "Epoch: 26, Training time: 170.63s, Loss: 168.6231\n",
      "Evaluate current model:\n",
      " Epoch: 26, Validation time: 32.43s \n",
      " Loss: 168.6231: \n",
      " Recall@20: 0.0608 \n",
      " NDCG@20: 0.1306\n",
      "Epoch: 27, Training time: 170.41s, Loss: 166.8185\n",
      "Evaluate current model:\n",
      " Epoch: 27, Validation time: 32.48s \n",
      " Loss: 166.8185: \n",
      " Recall@20: 0.0612 \n",
      " NDCG@20: 0.1323\n",
      "Epoch: 28, Training time: 170.14s, Loss: 165.3265\n",
      "Evaluate current model:\n",
      " Epoch: 28, Validation time: 32.39s \n",
      " Loss: 165.3265: \n",
      " Recall@20: 0.0626 \n",
      " NDCG@20: 0.1323\n",
      "Epoch: 29, Training time: 170.71s, Loss: 163.5929\n",
      "Evaluate current model:\n",
      " Epoch: 29, Validation time: 32.43s \n",
      " Loss: 163.5929: \n",
      " Recall@20: 0.0640 \n",
      " NDCG@20: 0.1370\n",
      "Epoch: 30, Training time: 170.75s, Loss: 161.9084\n",
      "Evaluate current model:\n",
      " Epoch: 30, Validation time: 32.38s \n",
      " Loss: 161.9084: \n",
      " Recall@20: 0.0652 \n",
      " NDCG@20: 0.1413\n",
      "Epoch: 31, Training time: 170.56s, Loss: 160.0646\n",
      "Evaluate current model:\n",
      " Epoch: 31, Validation time: 32.38s \n",
      " Loss: 160.0646: \n",
      " Recall@20: 0.0650 \n",
      " NDCG@20: 0.1399\n",
      "Epoch: 32, Training time: 170.78s, Loss: 158.9232\n",
      "Evaluate current model:\n",
      " Epoch: 32, Validation time: 32.39s \n",
      " Loss: 158.9232: \n",
      " Recall@20: 0.0670 \n",
      " NDCG@20: 0.1431\n",
      "Epoch: 33, Training time: 170.12s, Loss: 157.4657\n",
      "Evaluate current model:\n",
      " Epoch: 33, Validation time: 32.31s \n",
      " Loss: 157.4657: \n",
      " Recall@20: 0.0681 \n",
      " NDCG@20: 0.1492\n",
      "Epoch: 34, Training time: 170.45s, Loss: 155.5966\n",
      "Evaluate current model:\n",
      " Epoch: 34, Validation time: 32.33s \n",
      " Loss: 155.5966: \n",
      " Recall@20: 0.0702 \n",
      " NDCG@20: 0.1538\n",
      "Epoch: 35, Training time: 170.46s, Loss: 153.6931\n",
      "Evaluate current model:\n",
      " Epoch: 35, Validation time: 32.54s \n",
      " Loss: 153.6931: \n",
      " Recall@20: 0.0712 \n",
      " NDCG@20: 0.1535\n",
      "Epoch: 36, Training time: 170.48s, Loss: 152.2058\n",
      "Evaluate current model:\n",
      " Epoch: 36, Validation time: 32.44s \n",
      " Loss: 152.2058: \n",
      " Recall@20: 0.0725 \n",
      " NDCG@20: 0.1610\n",
      "Epoch: 37, Training time: 170.10s, Loss: 151.0614\n",
      "Evaluate current model:\n",
      " Epoch: 37, Validation time: 32.46s \n",
      " Loss: 151.0614: \n",
      " Recall@20: 0.0753 \n",
      " NDCG@20: 0.1667\n",
      "Epoch: 38, Training time: 170.80s, Loss: 149.0313\n",
      "Evaluate current model:\n",
      " Epoch: 38, Validation time: 32.26s \n",
      " Loss: 149.0313: \n",
      " Recall@20: 0.0747 \n",
      " NDCG@20: 0.1624\n",
      "Epoch: 39, Training time: 170.08s, Loss: 147.6750\n",
      "Evaluate current model:\n",
      " Epoch: 39, Validation time: 32.42s \n",
      " Loss: 147.6750: \n",
      " Recall@20: 0.0766 \n",
      " NDCG@20: 0.1704\n",
      "Epoch: 40, Training time: 170.40s, Loss: 146.4964\n",
      "Evaluate current model:\n",
      " Epoch: 40, Validation time: 32.36s \n",
      " Loss: 146.4964: \n",
      " Recall@20: 0.0783 \n",
      " NDCG@20: 0.1752\n",
      "Epoch: 41, Training time: 170.47s, Loss: 144.5005\n",
      "Evaluate current model:\n",
      " Epoch: 41, Validation time: 32.46s \n",
      " Loss: 144.5005: \n",
      " Recall@20: 0.0809 \n",
      " NDCG@20: 0.1793\n",
      "Epoch: 42, Training time: 170.31s, Loss: 143.2175\n",
      "Evaluate current model:\n",
      " Epoch: 42, Validation time: 32.49s \n",
      " Loss: 143.2175: \n",
      " Recall@20: 0.0821 \n",
      " NDCG@20: 0.1838\n",
      "Epoch: 43, Training time: 169.97s, Loss: 141.5228\n",
      "Evaluate current model:\n",
      " Epoch: 43, Validation time: 32.39s \n",
      " Loss: 141.5228: \n",
      " Recall@20: 0.0819 \n",
      " NDCG@20: 0.1816\n",
      "Epoch: 44, Training time: 170.48s, Loss: 139.6958\n",
      "Evaluate current model:\n",
      " Epoch: 44, Validation time: 32.35s \n",
      " Loss: 139.6958: \n",
      " Recall@20: 0.0840 \n",
      " NDCG@20: 0.1878\n",
      "Epoch: 45, Training time: 170.62s, Loss: 138.2106\n",
      "Evaluate current model:\n",
      " Epoch: 45, Validation time: 32.36s \n",
      " Loss: 138.2106: \n",
      " Recall@20: 0.0868 \n",
      " NDCG@20: 0.1908\n",
      "Epoch: 46, Training time: 170.39s, Loss: 136.9498\n",
      "Evaluate current model:\n",
      " Epoch: 46, Validation time: 32.47s \n",
      " Loss: 136.9498: \n",
      " Recall@20: 0.0874 \n",
      " NDCG@20: 0.1952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Training time: 170.48s, Loss: 135.2102\n",
      "Evaluate current model:\n",
      " Epoch: 47, Validation time: 32.38s \n",
      " Loss: 135.2102: \n",
      " Recall@20: 0.0875 \n",
      " NDCG@20: 0.1941\n",
      "Epoch: 48, Training time: 170.55s, Loss: 133.7819\n",
      "Evaluate current model:\n",
      " Epoch: 48, Validation time: 32.37s \n",
      " Loss: 133.7819: \n",
      " Recall@20: 0.0902 \n",
      " NDCG@20: 0.2001\n",
      "Epoch: 49, Training time: 170.30s, Loss: 131.7808\n",
      "Evaluate current model:\n",
      " Epoch: 49, Validation time: 32.43s \n",
      " Loss: 131.7808: \n",
      " Recall@20: 0.0914 \n",
      " NDCG@20: 0.2023\n",
      "Epoch: 50, Training time: 170.42s, Loss: 130.3409\n",
      "Evaluate current model:\n",
      " Epoch: 50, Validation time: 32.45s \n",
      " Loss: 130.3409: \n",
      " Recall@20: 0.0918 \n",
      " NDCG@20: 0.2037\n",
      "Epoch: 51, Training time: 170.01s, Loss: 128.6591\n",
      "Evaluate current model:\n",
      " Epoch: 51, Validation time: 32.35s \n",
      " Loss: 128.6591: \n",
      " Recall@20: 0.0942 \n",
      " NDCG@20: 0.2091\n",
      "Epoch: 52, Training time: 169.90s, Loss: 126.7971\n",
      "Evaluate current model:\n",
      " Epoch: 52, Validation time: 32.83s \n",
      " Loss: 126.7971: \n",
      " Recall@20: 0.0946 \n",
      " NDCG@20: 0.2091\n",
      "Epoch: 53, Training time: 170.29s, Loss: 124.8869\n",
      "Evaluate current model:\n",
      " Epoch: 53, Validation time: 32.77s \n",
      " Loss: 124.8869: \n",
      " Recall@20: 0.0957 \n",
      " NDCG@20: 0.2137\n",
      "Epoch: 54, Training time: 170.60s, Loss: 123.7038\n",
      "Evaluate current model:\n",
      " Epoch: 54, Validation time: 32.51s \n",
      " Loss: 123.7038: \n",
      " Recall@20: 0.0976 \n",
      " NDCG@20: 0.2145\n",
      "Epoch: 55, Training time: 170.33s, Loss: 121.6462\n",
      "Evaluate current model:\n",
      " Epoch: 55, Validation time: 32.90s \n",
      " Loss: 121.6462: \n",
      " Recall@20: 0.0979 \n",
      " NDCG@20: 0.2166\n",
      "Epoch: 56, Training time: 170.39s, Loss: 120.3254\n",
      "Evaluate current model:\n",
      " Epoch: 56, Validation time: 32.43s \n",
      " Loss: 120.3254: \n",
      " Recall@20: 0.0990 \n",
      " NDCG@20: 0.2194\n",
      "Epoch: 57, Training time: 170.20s, Loss: 118.5205\n",
      "Evaluate current model:\n",
      " Epoch: 57, Validation time: 32.52s \n",
      " Loss: 118.5205: \n",
      " Recall@20: 0.0994 \n",
      " NDCG@20: 0.2181\n",
      "Epoch: 58, Training time: 170.23s, Loss: 116.7342\n",
      "Evaluate current model:\n",
      " Epoch: 58, Validation time: 32.66s \n",
      " Loss: 116.7342: \n",
      " Recall@20: 0.1003 \n",
      " NDCG@20: 0.2208\n",
      "Epoch: 59, Training time: 170.00s, Loss: 115.2604\n",
      "Evaluate current model:\n",
      " Epoch: 59, Validation time: 32.44s \n",
      " Loss: 115.2604: \n",
      " Recall@20: 0.1014 \n",
      " NDCG@20: 0.2255\n",
      "Epoch: 60, Training time: 170.29s, Loss: 113.3331\n",
      "Evaluate current model:\n",
      " Epoch: 60, Validation time: 32.55s \n",
      " Loss: 113.3331: \n",
      " Recall@20: 0.1020 \n",
      " NDCG@20: 0.2248\n",
      "Epoch: 61, Training time: 169.97s, Loss: 112.1983\n",
      "Evaluate current model:\n",
      " Epoch: 61, Validation time: 32.93s \n",
      " Loss: 112.1983: \n",
      " Recall@20: 0.1030 \n",
      " NDCG@20: 0.2267\n",
      "Epoch: 62, Training time: 170.15s, Loss: 110.0743\n",
      "Evaluate current model:\n",
      " Epoch: 62, Validation time: 32.48s \n",
      " Loss: 110.0743: \n",
      " Recall@20: 0.1030 \n",
      " NDCG@20: 0.2285\n",
      "Epoch: 63, Training time: 170.25s, Loss: 108.3165\n",
      "Evaluate current model:\n",
      " Epoch: 63, Validation time: 32.52s \n",
      " Loss: 108.3165: \n",
      " Recall@20: 0.1038 \n",
      " NDCG@20: 0.2294\n",
      "Epoch: 64, Training time: 170.59s, Loss: 106.7297\n",
      "Evaluate current model:\n",
      " Epoch: 64, Validation time: 32.46s \n",
      " Loss: 106.7297: \n",
      " Recall@20: 0.1043 \n",
      " NDCG@20: 0.2294\n",
      "Epoch: 65, Training time: 170.00s, Loss: 105.1926\n",
      "Evaluate current model:\n",
      " Epoch: 65, Validation time: 32.48s \n",
      " Loss: 105.1926: \n",
      " Recall@20: 0.1052 \n",
      " NDCG@20: 0.2330\n",
      "Epoch: 66, Training time: 170.49s, Loss: 103.5280\n",
      "Evaluate current model:\n",
      " Epoch: 66, Validation time: 32.62s \n",
      " Loss: 103.5280: \n",
      " Recall@20: 0.1053 \n",
      " NDCG@20: 0.2322\n",
      "Epoch: 67, Training time: 170.55s, Loss: 101.7931\n",
      "Evaluate current model:\n",
      " Epoch: 67, Validation time: 32.47s \n",
      " Loss: 101.7931: \n",
      " Recall@20: 0.1067 \n",
      " NDCG@20: 0.2353\n",
      "Epoch: 68, Training time: 170.51s, Loss: 100.0307\n",
      "Evaluate current model:\n",
      " Epoch: 68, Validation time: 32.48s \n",
      " Loss: 100.0307: \n",
      " Recall@20: 0.1063 \n",
      " NDCG@20: 0.2350\n",
      "Epoch: 69, Training time: 170.17s, Loss: 98.6487\n",
      "Evaluate current model:\n",
      " Epoch: 69, Validation time: 32.66s \n",
      " Loss: 98.6487: \n",
      " Recall@20: 0.1077 \n",
      " NDCG@20: 0.2358\n",
      "Epoch: 70, Training time: 170.38s, Loss: 97.1793\n",
      "Evaluate current model:\n",
      " Epoch: 70, Validation time: 32.46s \n",
      " Loss: 97.1793: \n",
      " Recall@20: 0.1073 \n",
      " NDCG@20: 0.2368\n",
      "Epoch: 71, Training time: 170.17s, Loss: 95.3374\n",
      "Evaluate current model:\n",
      " Epoch: 71, Validation time: 32.63s \n",
      " Loss: 95.3374: \n",
      " Recall@20: 0.1085 \n",
      " NDCG@20: 0.2382\n",
      "Epoch: 72, Training time: 170.06s, Loss: 93.9123\n",
      "Evaluate current model:\n",
      " Epoch: 72, Validation time: 32.37s \n",
      " Loss: 93.9123: \n",
      " Recall@20: 0.1092 \n",
      " NDCG@20: 0.2398\n",
      "Epoch: 73, Training time: 170.40s, Loss: 92.5558\n",
      "Evaluate current model:\n",
      " Epoch: 73, Validation time: 32.59s \n",
      " Loss: 92.5558: \n",
      " Recall@20: 0.1091 \n",
      " NDCG@20: 0.2408\n",
      "Epoch: 74, Training time: 170.19s, Loss: 91.2759\n",
      "Evaluate current model:\n",
      " Epoch: 74, Validation time: 32.57s \n",
      " Loss: 91.2759: \n",
      " Recall@20: 0.1099 \n",
      " NDCG@20: 0.2410\n",
      "Epoch: 75, Training time: 170.04s, Loss: 89.2261\n",
      "Evaluate current model:\n",
      " Epoch: 75, Validation time: 32.40s \n",
      " Loss: 89.2261: \n",
      " Recall@20: 0.1108 \n",
      " NDCG@20: 0.2426\n",
      "Epoch: 76, Training time: 170.28s, Loss: 87.6870\n",
      "Evaluate current model:\n",
      " Epoch: 76, Validation time: 32.50s \n",
      " Loss: 87.6870: \n",
      " Recall@20: 0.1105 \n",
      " NDCG@20: 0.2424\n",
      "Epoch: 77, Training time: 170.23s, Loss: 86.6738\n",
      "Evaluate current model:\n",
      " Epoch: 77, Validation time: 32.40s \n",
      " Loss: 86.6738: \n",
      " Recall@20: 0.1113 \n",
      " NDCG@20: 0.2431\n",
      "Epoch: 78, Training time: 170.33s, Loss: 85.1217\n",
      "Evaluate current model:\n",
      " Epoch: 78, Validation time: 32.53s \n",
      " Loss: 85.1217: \n",
      " Recall@20: 0.1116 \n",
      " NDCG@20: 0.2443\n",
      "Epoch: 79, Training time: 170.34s, Loss: 83.9718\n",
      "Evaluate current model:\n",
      " Epoch: 79, Validation time: 32.66s \n",
      " Loss: 83.9718: \n",
      " Recall@20: 0.1118 \n",
      " NDCG@20: 0.2451\n",
      "Epoch: 80, Training time: 170.34s, Loss: 82.4327\n",
      "Evaluate current model:\n",
      " Epoch: 80, Validation time: 32.67s \n",
      " Loss: 82.4327: \n",
      " Recall@20: 0.1125 \n",
      " NDCG@20: 0.2444\n",
      "Epoch: 81, Training time: 170.27s, Loss: 80.8937\n",
      "Evaluate current model:\n",
      " Epoch: 81, Validation time: 32.58s \n",
      " Loss: 80.8937: \n",
      " Recall@20: 0.1125 \n",
      " NDCG@20: 0.2460\n",
      "Epoch: 82, Training time: 170.80s, Loss: 80.1448\n",
      "Evaluate current model:\n",
      " Epoch: 82, Validation time: 32.60s \n",
      " Loss: 80.1448: \n",
      " Recall@20: 0.1125 \n",
      " NDCG@20: 0.2460\n",
      "Epoch: 83, Training time: 170.57s, Loss: 78.5282\n",
      "Evaluate current model:\n",
      " Epoch: 83, Validation time: 32.59s \n",
      " Loss: 78.5282: \n",
      " Recall@20: 0.1130 \n",
      " NDCG@20: 0.2462\n",
      "Epoch: 84, Training time: 170.24s, Loss: 77.6212\n",
      "Evaluate current model:\n",
      " Epoch: 84, Validation time: 32.58s \n",
      " Loss: 77.6212: \n",
      " Recall@20: 0.1128 \n",
      " NDCG@20: 0.2471\n",
      "Epoch: 85, Training time: 170.77s, Loss: 76.2869\n",
      "Evaluate current model:\n",
      " Epoch: 85, Validation time: 32.46s \n",
      " Loss: 76.2869: \n",
      " Recall@20: 0.1137 \n",
      " NDCG@20: 0.2485\n",
      "Epoch: 86, Training time: 170.22s, Loss: 75.1472\n",
      "Evaluate current model:\n",
      " Epoch: 86, Validation time: 32.57s \n",
      " Loss: 75.1472: \n",
      " Recall@20: 0.1135 \n",
      " NDCG@20: 0.2482\n",
      "Epoch: 87, Training time: 170.03s, Loss: 74.2829\n",
      "Evaluate current model:\n",
      " Epoch: 87, Validation time: 32.54s \n",
      " Loss: 74.2829: \n",
      " Recall@20: 0.1139 \n",
      " NDCG@20: 0.2478\n",
      "Epoch: 88, Training time: 170.56s, Loss: 72.7494\n",
      "Evaluate current model:\n",
      " Epoch: 88, Validation time: 32.53s \n",
      " Loss: 72.7494: \n",
      " Recall@20: 0.1142 \n",
      " NDCG@20: 0.2489\n",
      "Epoch: 89, Training time: 170.16s, Loss: 71.6115\n",
      "Evaluate current model:\n",
      " Epoch: 89, Validation time: 32.40s \n",
      " Loss: 71.6115: \n",
      " Recall@20: 0.1147 \n",
      " NDCG@20: 0.2497\n",
      "Epoch: 90, Training time: 170.41s, Loss: 71.0139\n",
      "Evaluate current model:\n",
      " Epoch: 90, Validation time: 32.57s \n",
      " Loss: 71.0139: \n",
      " Recall@20: 0.1150 \n",
      " NDCG@20: 0.2502\n",
      "Epoch: 91, Training time: 170.58s, Loss: 69.7331\n",
      "Evaluate current model:\n",
      " Epoch: 91, Validation time: 32.51s \n",
      " Loss: 69.7331: \n",
      " Recall@20: 0.1148 \n",
      " NDCG@20: 0.2505\n",
      "Epoch: 92, Training time: 170.06s, Loss: 68.9799\n",
      "Evaluate current model:\n",
      " Epoch: 92, Validation time: 32.56s \n",
      " Loss: 68.9799: \n",
      " Recall@20: 0.1157 \n",
      " NDCG@20: 0.2511\n",
      "Epoch: 93, Training time: 170.32s, Loss: 67.7018\n",
      "Evaluate current model:\n",
      " Epoch: 93, Validation time: 32.55s \n",
      " Loss: 67.7018: \n",
      " Recall@20: 0.1154 \n",
      " NDCG@20: 0.2522\n",
      "Epoch: 94, Training time: 170.16s, Loss: 66.5437\n",
      "Evaluate current model:\n",
      " Epoch: 94, Validation time: 32.78s \n",
      " Loss: 66.5437: \n",
      " Recall@20: 0.1158 \n",
      " NDCG@20: 0.2519\n",
      "Epoch: 95, Training time: 170.18s, Loss: 66.0005\n",
      "Evaluate current model:\n",
      " Epoch: 95, Validation time: 32.43s \n",
      " Loss: 66.0005: \n",
      " Recall@20: 0.1164 \n",
      " NDCG@20: 0.2526\n",
      "Epoch: 96, Training time: 169.84s, Loss: 65.1126\n",
      "Evaluate current model:\n",
      " Epoch: 96, Validation time: 32.45s \n",
      " Loss: 65.1126: \n",
      " Recall@20: 0.1163 \n",
      " NDCG@20: 0.2529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Training time: 170.30s, Loss: 64.3006\n",
      "Evaluate current model:\n",
      " Epoch: 97, Validation time: 32.57s \n",
      " Loss: 64.3006: \n",
      " Recall@20: 0.1168 \n",
      " NDCG@20: 0.2535\n",
      "Epoch: 98, Training time: 170.16s, Loss: 62.9493\n",
      "Evaluate current model:\n",
      " Epoch: 98, Validation time: 32.56s \n",
      " Loss: 62.9493: \n",
      " Recall@20: 0.1172 \n",
      " NDCG@20: 0.2535\n",
      "Epoch: 99, Training time: 170.35s, Loss: 62.3266\n",
      "Evaluate current model:\n",
      " Epoch: 99, Validation time: 32.60s \n",
      " Loss: 62.3266: \n",
      " Recall@20: 0.1174 \n",
      " NDCG@20: 0.2553\n",
      "Epoch: 100, Training time: 170.57s, Loss: 61.7146\n",
      "Evaluate current model:\n",
      " Epoch: 100, Validation time: 32.69s \n",
      " Loss: 61.7146: \n",
      " Recall@20: 0.1173 \n",
      " NDCG@20: 0.2554\n",
      "Epoch: 101, Training time: 169.80s, Loss: 60.5852\n",
      "Evaluate current model:\n",
      " Epoch: 101, Validation time: 32.52s \n",
      " Loss: 60.5852: \n",
      " Recall@20: 0.1177 \n",
      " NDCG@20: 0.2560\n",
      "Epoch: 102, Training time: 170.52s, Loss: 59.9454\n",
      "Evaluate current model:\n",
      " Epoch: 102, Validation time: 32.49s \n",
      " Loss: 59.9454: \n",
      " Recall@20: 0.1182 \n",
      " NDCG@20: 0.2554\n",
      "Epoch: 103, Training time: 170.02s, Loss: 59.0648\n",
      "Evaluate current model:\n",
      " Epoch: 103, Validation time: 33.31s \n",
      " Loss: 59.0648: \n",
      " Recall@20: 0.1182 \n",
      " NDCG@20: 0.2564\n",
      "Epoch: 104, Training time: 174.85s, Loss: 58.1459\n",
      "Evaluate current model:\n",
      " Epoch: 104, Validation time: 37.79s \n",
      " Loss: 58.1459: \n",
      " Recall@20: 0.1182 \n",
      " NDCG@20: 0.2557\n",
      "Epoch: 105, Training time: 170.24s, Loss: 57.3375\n",
      "Evaluate current model:\n",
      " Epoch: 105, Validation time: 32.70s \n",
      " Loss: 57.3375: \n",
      " Recall@20: 0.1185 \n",
      " NDCG@20: 0.2567\n",
      "Epoch: 106, Training time: 172.52s, Loss: 56.7860\n",
      "Evaluate current model:\n",
      " Epoch: 106, Validation time: 32.86s \n",
      " Loss: 56.7860: \n",
      " Recall@20: 0.1187 \n",
      " NDCG@20: 0.2570\n",
      "Epoch: 107, Training time: 170.21s, Loss: 56.0706\n",
      "Evaluate current model:\n",
      " Epoch: 107, Validation time: 33.03s \n",
      " Loss: 56.0706: \n",
      " Recall@20: 0.1186 \n",
      " NDCG@20: 0.2573\n",
      "Epoch: 108, Training time: 169.88s, Loss: 55.0921\n",
      "Evaluate current model:\n",
      " Epoch: 108, Validation time: 33.13s \n",
      " Loss: 55.0921: \n",
      " Recall@20: 0.1192 \n",
      " NDCG@20: 0.2581\n",
      "Epoch: 109, Training time: 170.55s, Loss: 54.8299\n",
      "Evaluate current model:\n",
      " Epoch: 109, Validation time: 33.00s \n",
      " Loss: 54.8299: \n",
      " Recall@20: 0.1192 \n",
      " NDCG@20: 0.2579\n",
      "Epoch: 110, Training time: 170.03s, Loss: 54.3280\n",
      "Evaluate current model:\n",
      " Epoch: 110, Validation time: 32.68s \n",
      " Loss: 54.3280: \n",
      " Recall@20: 0.1197 \n",
      " NDCG@20: 0.2577\n",
      "Epoch: 111, Training time: 170.27s, Loss: 53.5048\n",
      "Evaluate current model:\n",
      " Epoch: 111, Validation time: 32.76s \n",
      " Loss: 53.5048: \n",
      " Recall@20: 0.1198 \n",
      " NDCG@20: 0.2583\n",
      "Epoch: 112, Training time: 170.24s, Loss: 52.8773\n",
      "Evaluate current model:\n",
      " Epoch: 112, Validation time: 32.78s \n",
      " Loss: 52.8773: \n",
      " Recall@20: 0.1202 \n",
      " NDCG@20: 0.2592\n",
      "Epoch: 113, Training time: 170.43s, Loss: 52.3919\n",
      "Evaluate current model:\n",
      " Epoch: 113, Validation time: 32.80s \n",
      " Loss: 52.3919: \n",
      " Recall@20: 0.1202 \n",
      " NDCG@20: 0.2595\n",
      "Epoch: 114, Training time: 170.39s, Loss: 51.9737\n",
      "Evaluate current model:\n",
      " Epoch: 114, Validation time: 32.85s \n",
      " Loss: 51.9737: \n",
      " Recall@20: 0.1210 \n",
      " NDCG@20: 0.2605\n",
      "Epoch: 115, Training time: 170.39s, Loss: 51.4735\n",
      "Evaluate current model:\n",
      " Epoch: 115, Validation time: 32.77s \n",
      " Loss: 51.4735: \n",
      " Recall@20: 0.1203 \n",
      " NDCG@20: 0.2598\n",
      "Epoch: 116, Training time: 170.21s, Loss: 50.3856\n",
      "Evaluate current model:\n",
      " Epoch: 116, Validation time: 32.83s \n",
      " Loss: 50.3856: \n",
      " Recall@20: 0.1210 \n",
      " NDCG@20: 0.2613\n",
      "Epoch: 117, Training time: 170.28s, Loss: 50.3239\n",
      "Evaluate current model:\n",
      " Epoch: 117, Validation time: 32.93s \n",
      " Loss: 50.3239: \n",
      " Recall@20: 0.1214 \n",
      " NDCG@20: 0.2617\n",
      "Epoch: 118, Training time: 170.30s, Loss: 49.5852\n",
      "Evaluate current model:\n",
      " Epoch: 118, Validation time: 33.00s \n",
      " Loss: 49.5852: \n",
      " Recall@20: 0.1208 \n",
      " NDCG@20: 0.2609\n",
      "Epoch: 119, Training time: 170.11s, Loss: 49.1085\n",
      "Evaluate current model:\n",
      " Epoch: 119, Validation time: 33.01s \n",
      " Loss: 49.1085: \n",
      " Recall@20: 0.1212 \n",
      " NDCG@20: 0.2614\n",
      "Epoch: 120, Training time: 170.11s, Loss: 48.6487\n",
      "Evaluate current model:\n",
      " Epoch: 120, Validation time: 32.97s \n",
      " Loss: 48.6487: \n",
      " Recall@20: 0.1211 \n",
      " NDCG@20: 0.2609\n",
      "Epoch: 121, Training time: 170.23s, Loss: 48.0424\n",
      "Evaluate current model:\n",
      " Epoch: 121, Validation time: 32.99s \n",
      " Loss: 48.0424: \n",
      " Recall@20: 0.1213 \n",
      " NDCG@20: 0.2616\n",
      "Epoch: 122, Training time: 170.02s, Loss: 47.3959\n",
      "Evaluate current model:\n",
      " Epoch: 122, Validation time: 32.94s \n",
      " Loss: 47.3959: \n",
      " Recall@20: 0.1222 \n",
      " NDCG@20: 0.2623\n",
      "Epoch: 123, Training time: 170.28s, Loss: 46.7110\n",
      "Evaluate current model:\n",
      " Epoch: 123, Validation time: 32.80s \n",
      " Loss: 46.7110: \n",
      " Recall@20: 0.1218 \n",
      " NDCG@20: 0.2617\n",
      "Epoch: 124, Training time: 170.48s, Loss: 46.5677\n",
      "Evaluate current model:\n",
      " Epoch: 124, Validation time: 32.91s \n",
      " Loss: 46.5677: \n",
      " Recall@20: 0.1224 \n",
      " NDCG@20: 0.2636\n",
      "Epoch: 125, Training time: 170.26s, Loss: 45.9086\n",
      "Evaluate current model:\n",
      " Epoch: 125, Validation time: 32.72s \n",
      " Loss: 45.9086: \n",
      " Recall@20: 0.1224 \n",
      " NDCG@20: 0.2630\n",
      "Epoch: 126, Training time: 170.12s, Loss: 45.6410\n",
      "Evaluate current model:\n",
      " Epoch: 126, Validation time: 32.94s \n",
      " Loss: 45.6410: \n",
      " Recall@20: 0.1233 \n",
      " NDCG@20: 0.2634\n",
      "Epoch: 127, Training time: 170.39s, Loss: 45.1123\n",
      "Evaluate current model:\n",
      " Epoch: 127, Validation time: 32.89s \n",
      " Loss: 45.1123: \n",
      " Recall@20: 0.1223 \n",
      " NDCG@20: 0.2624\n",
      "Epoch: 128, Training time: 170.07s, Loss: 44.6941\n",
      "Evaluate current model:\n",
      " Epoch: 128, Validation time: 32.90s \n",
      " Loss: 44.6941: \n",
      " Recall@20: 0.1226 \n",
      " NDCG@20: 0.2635\n",
      "Epoch: 129, Training time: 170.88s, Loss: 44.2259\n",
      "Evaluate current model:\n",
      " Epoch: 129, Validation time: 32.89s \n",
      " Loss: 44.2259: \n",
      " Recall@20: 0.1235 \n",
      " NDCG@20: 0.2641\n",
      "Epoch: 130, Training time: 170.36s, Loss: 44.0240\n",
      "Evaluate current model:\n",
      " Epoch: 130, Validation time: 32.86s \n",
      " Loss: 44.0240: \n",
      " Recall@20: 0.1237 \n",
      " NDCG@20: 0.2643\n",
      "Epoch: 131, Training time: 170.16s, Loss: 43.4091\n",
      "Evaluate current model:\n",
      " Epoch: 131, Validation time: 32.95s \n",
      " Loss: 43.4091: \n",
      " Recall@20: 0.1235 \n",
      " NDCG@20: 0.2640\n",
      "Epoch: 132, Training time: 170.16s, Loss: 43.0805\n",
      "Evaluate current model:\n",
      " Epoch: 132, Validation time: 32.96s \n",
      " Loss: 43.0805: \n",
      " Recall@20: 0.1242 \n",
      " NDCG@20: 0.2656\n",
      "Epoch: 133, Training time: 170.39s, Loss: 42.4728\n",
      "Evaluate current model:\n",
      " Epoch: 133, Validation time: 32.85s \n",
      " Loss: 42.4728: \n",
      " Recall@20: 0.1243 \n",
      " NDCG@20: 0.2648\n",
      "Epoch: 134, Training time: 170.06s, Loss: 42.1363\n",
      "Evaluate current model:\n",
      " Epoch: 134, Validation time: 32.97s \n",
      " Loss: 42.1363: \n",
      " Recall@20: 0.1244 \n",
      " NDCG@20: 0.2663\n",
      "Epoch: 135, Training time: 170.18s, Loss: 42.2537\n",
      "Evaluate current model:\n",
      " Epoch: 135, Validation time: 32.87s \n",
      " Loss: 42.2537: \n",
      " Recall@20: 0.1249 \n",
      " NDCG@20: 0.2668\n",
      "Epoch: 136, Training time: 170.18s, Loss: 41.7136\n",
      "Evaluate current model:\n",
      " Epoch: 136, Validation time: 32.86s \n",
      " Loss: 41.7136: \n",
      " Recall@20: 0.1245 \n",
      " NDCG@20: 0.2664\n",
      "Epoch: 137, Training time: 169.95s, Loss: 41.4512\n",
      "Evaluate current model:\n",
      " Epoch: 137, Validation time: 32.86s \n",
      " Loss: 41.4512: \n",
      " Recall@20: 0.1250 \n",
      " NDCG@20: 0.2668\n",
      "Epoch: 138, Training time: 170.28s, Loss: 40.7987\n",
      "Evaluate current model:\n",
      " Epoch: 138, Validation time: 32.88s \n",
      " Loss: 40.7987: \n",
      " Recall@20: 0.1260 \n",
      " NDCG@20: 0.2675\n",
      "Epoch: 139, Training time: 170.52s, Loss: 40.7249\n",
      "Evaluate current model:\n",
      " Epoch: 139, Validation time: 32.77s \n",
      " Loss: 40.7249: \n",
      " Recall@20: 0.1255 \n",
      " NDCG@20: 0.2674\n",
      "Epoch: 140, Training time: 170.34s, Loss: 40.0175\n",
      "Evaluate current model:\n",
      " Epoch: 140, Validation time: 32.79s \n",
      " Loss: 40.0175: \n",
      " Recall@20: 0.1253 \n",
      " NDCG@20: 0.2670\n",
      "Epoch: 141, Training time: 170.10s, Loss: 39.6497\n",
      "Evaluate current model:\n",
      " Epoch: 141, Validation time: 32.94s \n",
      " Loss: 39.6497: \n",
      " Recall@20: 0.1256 \n",
      " NDCG@20: 0.2677\n",
      "Epoch: 142, Training time: 170.46s, Loss: 39.5288\n",
      "Evaluate current model:\n",
      " Epoch: 142, Validation time: 32.92s \n",
      " Loss: 39.5288: \n",
      " Recall@20: 0.1256 \n",
      " NDCG@20: 0.2677\n",
      "Epoch: 143, Training time: 170.36s, Loss: 39.0225\n",
      "Evaluate current model:\n",
      " Epoch: 143, Validation time: 32.85s \n",
      " Loss: 39.0225: \n",
      " Recall@20: 0.1266 \n",
      " NDCG@20: 0.2691\n",
      "Epoch: 144, Training time: 172.73s, Loss: 38.9590\n",
      "Evaluate current model:\n",
      " Epoch: 144, Validation time: 33.01s \n",
      " Loss: 38.9590: \n",
      " Recall@20: 0.1261 \n",
      " NDCG@20: 0.2682\n",
      "Epoch: 145, Training time: 171.52s, Loss: 38.4305\n",
      "Evaluate current model:\n",
      " Epoch: 145, Validation time: 32.75s \n",
      " Loss: 38.4305: \n",
      " Recall@20: 0.1267 \n",
      " NDCG@20: 0.2691\n",
      "Epoch: 146, Training time: 170.22s, Loss: 38.1759\n",
      "Evaluate current model:\n",
      " Epoch: 146, Validation time: 32.80s \n",
      " Loss: 38.1759: \n",
      " Recall@20: 0.1265 \n",
      " NDCG@20: 0.2688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147, Training time: 170.50s, Loss: 37.9937\n",
      "Evaluate current model:\n",
      " Epoch: 147, Validation time: 32.81s \n",
      " Loss: 37.9937: \n",
      " Recall@20: 0.1270 \n",
      " NDCG@20: 0.2695\n",
      "Epoch: 148, Training time: 170.42s, Loss: 37.3742\n",
      "Evaluate current model:\n",
      " Epoch: 148, Validation time: 33.01s \n",
      " Loss: 37.3742: \n",
      " Recall@20: 0.1271 \n",
      " NDCG@20: 0.2701\n",
      "Epoch: 149, Training time: 170.31s, Loss: 37.2637\n",
      "Evaluate current model:\n",
      " Epoch: 149, Validation time: 32.84s \n",
      " Loss: 37.2637: \n",
      " Recall@20: 0.1271 \n",
      " NDCG@20: 0.2696\n",
      "Epoch: 150, Training time: 170.28s, Loss: 36.7690\n",
      "Evaluate current model:\n",
      " Epoch: 150, Validation time: 32.90s \n",
      " Loss: 36.7690: \n",
      " Recall@20: 0.1275 \n",
      " NDCG@20: 0.2700\n",
      "Epoch: 151, Training time: 169.95s, Loss: 36.6697\n",
      "Evaluate current model:\n",
      " Epoch: 151, Validation time: 32.83s \n",
      " Loss: 36.6697: \n",
      " Recall@20: 0.1277 \n",
      " NDCG@20: 0.2690\n",
      "Epoch: 152, Training time: 170.48s, Loss: 36.7265\n",
      "Evaluate current model:\n",
      " Epoch: 152, Validation time: 32.90s \n",
      " Loss: 36.7265: \n",
      " Recall@20: 0.1278 \n",
      " NDCG@20: 0.2717\n",
      "Epoch: 153, Training time: 169.93s, Loss: 35.9995\n",
      "Evaluate current model:\n",
      " Epoch: 153, Validation time: 32.88s \n",
      " Loss: 35.9995: \n",
      " Recall@20: 0.1281 \n",
      " NDCG@20: 0.2710\n",
      "Epoch: 154, Training time: 170.40s, Loss: 35.6125\n",
      "Evaluate current model:\n",
      " Epoch: 154, Validation time: 32.95s \n",
      " Loss: 35.6125: \n",
      " Recall@20: 0.1276 \n",
      " NDCG@20: 0.2702\n",
      "Epoch: 155, Training time: 170.06s, Loss: 35.4247\n",
      "Evaluate current model:\n",
      " Epoch: 155, Validation time: 32.87s \n",
      " Loss: 35.4247: \n",
      " Recall@20: 0.1285 \n",
      " NDCG@20: 0.2716\n",
      "Epoch: 156, Training time: 170.32s, Loss: 35.2805\n",
      "Evaluate current model:\n",
      " Epoch: 156, Validation time: 32.88s \n",
      " Loss: 35.2805: \n",
      " Recall@20: 0.1276 \n",
      " NDCG@20: 0.2719\n",
      "Epoch: 157, Training time: 170.20s, Loss: 35.1920\n",
      "Evaluate current model:\n",
      " Epoch: 157, Validation time: 32.76s \n",
      " Loss: 35.1920: \n",
      " Recall@20: 0.1285 \n",
      " NDCG@20: 0.2726\n",
      "Epoch: 158, Training time: 170.55s, Loss: 34.5081\n",
      "Evaluate current model:\n",
      " Epoch: 158, Validation time: 32.79s \n",
      " Loss: 34.5081: \n",
      " Recall@20: 0.1277 \n",
      " NDCG@20: 0.2719\n",
      "Epoch: 159, Training time: 170.62s, Loss: 34.2035\n",
      "Evaluate current model:\n",
      " Epoch: 159, Validation time: 33.10s \n",
      " Loss: 34.2035: \n",
      " Recall@20: 0.1289 \n",
      " NDCG@20: 0.2717\n",
      "Epoch: 160, Training time: 170.41s, Loss: 34.2883\n",
      "Evaluate current model:\n",
      " Epoch: 160, Validation time: 32.77s \n",
      " Loss: 34.2883: \n",
      " Recall@20: 0.1285 \n",
      " NDCG@20: 0.2709\n",
      "Epoch: 161, Training time: 170.09s, Loss: 33.9160\n",
      "Evaluate current model:\n",
      " Epoch: 161, Validation time: 32.82s \n",
      " Loss: 33.9160: \n",
      " Recall@20: 0.1287 \n",
      " NDCG@20: 0.2727\n",
      "Epoch: 162, Training time: 169.99s, Loss: 33.7747\n",
      "Evaluate current model:\n",
      " Epoch: 162, Validation time: 33.35s \n",
      " Loss: 33.7747: \n",
      " Recall@20: 0.1287 \n",
      " NDCG@20: 0.2726\n",
      "Epoch: 163, Training time: 170.48s, Loss: 33.4323\n",
      "Evaluate current model:\n",
      " Epoch: 163, Validation time: 32.91s \n",
      " Loss: 33.4323: \n",
      " Recall@20: 0.1297 \n",
      " NDCG@20: 0.2728\n",
      "Epoch: 164, Training time: 170.03s, Loss: 33.4093\n",
      "Evaluate current model:\n",
      " Epoch: 164, Validation time: 32.84s \n",
      " Loss: 33.4093: \n",
      " Recall@20: 0.1291 \n",
      " NDCG@20: 0.2724\n",
      "Epoch: 165, Training time: 170.10s, Loss: 33.0495\n",
      "Evaluate current model:\n",
      " Epoch: 165, Validation time: 32.94s \n",
      " Loss: 33.0495: \n",
      " Recall@20: 0.1302 \n",
      " NDCG@20: 0.2739\n",
      "Epoch: 166, Training time: 172.66s, Loss: 32.8489\n",
      "Evaluate current model:\n",
      " Epoch: 166, Validation time: 37.39s \n",
      " Loss: 32.8489: \n",
      " Recall@20: 0.1296 \n",
      " NDCG@20: 0.2737\n",
      "Epoch: 167, Training time: 173.92s, Loss: 32.7496\n",
      "Evaluate current model:\n",
      " Epoch: 167, Validation time: 33.32s \n",
      " Loss: 32.7496: \n",
      " Recall@20: 0.1297 \n",
      " NDCG@20: 0.2737\n",
      "Epoch: 168, Training time: 173.36s, Loss: 32.3836\n",
      "Evaluate current model:\n",
      " Epoch: 168, Validation time: 36.25s \n",
      " Loss: 32.3836: \n",
      " Recall@20: 0.1295 \n",
      " NDCG@20: 0.2735\n",
      "Epoch: 169, Training time: 179.41s, Loss: 32.0632\n",
      "Evaluate current model:\n",
      " Epoch: 169, Validation time: 35.85s \n",
      " Loss: 32.0632: \n",
      " Recall@20: 0.1298 \n",
      " NDCG@20: 0.2743\n",
      "Epoch: 170, Training time: 181.24s, Loss: 31.8006\n",
      "Evaluate current model:\n",
      " Epoch: 170, Validation time: 37.02s \n",
      " Loss: 31.8006: \n",
      " Recall@20: 0.1305 \n",
      " NDCG@20: 0.2741\n",
      "Epoch: 171, Training time: 182.90s, Loss: 31.8049\n",
      "Evaluate current model:\n",
      " Epoch: 171, Validation time: 37.71s \n",
      " Loss: 31.8049: \n",
      " Recall@20: 0.1309 \n",
      " NDCG@20: 0.2752\n",
      "Epoch: 172, Training time: 181.52s, Loss: 31.6865\n",
      "Evaluate current model:\n",
      " Epoch: 172, Validation time: 36.34s \n",
      " Loss: 31.6865: \n",
      " Recall@20: 0.1304 \n",
      " NDCG@20: 0.2754\n",
      "Epoch: 173, Training time: 173.93s, Loss: 31.1916\n",
      "Evaluate current model:\n",
      " Epoch: 173, Validation time: 35.59s \n",
      " Loss: 31.1916: \n",
      " Recall@20: 0.1308 \n",
      " NDCG@20: 0.2752\n",
      "Epoch: 174, Training time: 175.22s, Loss: 31.4336\n",
      "Evaluate current model:\n",
      " Epoch: 174, Validation time: 35.57s \n",
      " Loss: 31.4336: \n",
      " Recall@20: 0.1311 \n",
      " NDCG@20: 0.2755\n",
      "Epoch: 175, Training time: 178.39s, Loss: 31.2083\n",
      "Evaluate current model:\n",
      " Epoch: 175, Validation time: 33.50s \n",
      " Loss: 31.2083: \n",
      " Recall@20: 0.1309 \n",
      " NDCG@20: 0.2753\n",
      "Epoch: 176, Training time: 172.49s, Loss: 30.4354\n",
      "Evaluate current model:\n",
      " Epoch: 176, Validation time: 32.47s \n",
      " Loss: 30.4354: \n",
      " Recall@20: 0.1315 \n",
      " NDCG@20: 0.2754\n",
      "Epoch: 177, Training time: 170.05s, Loss: 30.6749\n",
      "Evaluate current model:\n",
      " Epoch: 177, Validation time: 32.36s \n",
      " Loss: 30.6749: \n",
      " Recall@20: 0.1310 \n",
      " NDCG@20: 0.2746\n",
      "Epoch: 178, Training time: 170.45s, Loss: 30.6126\n",
      "Evaluate current model:\n",
      " Epoch: 178, Validation time: 32.31s \n",
      " Loss: 30.6126: \n",
      " Recall@20: 0.1313 \n",
      " NDCG@20: 0.2756\n",
      "Epoch: 179, Training time: 171.24s, Loss: 30.3085\n",
      "Evaluate current model:\n",
      " Epoch: 179, Validation time: 32.34s \n",
      " Loss: 30.3085: \n",
      " Recall@20: 0.1317 \n",
      " NDCG@20: 0.2764\n",
      "Epoch: 180, Training time: 170.23s, Loss: 29.8171\n",
      "Evaluate current model:\n",
      " Epoch: 180, Validation time: 32.38s \n",
      " Loss: 29.8171: \n",
      " Recall@20: 0.1320 \n",
      " NDCG@20: 0.2758\n",
      "Epoch: 181, Training time: 170.70s, Loss: 29.7705\n",
      "Evaluate current model:\n",
      " Epoch: 181, Validation time: 32.43s \n",
      " Loss: 29.7705: \n",
      " Recall@20: 0.1321 \n",
      " NDCG@20: 0.2765\n",
      "Epoch: 182, Training time: 169.99s, Loss: 29.6771\n",
      "Evaluate current model:\n",
      " Epoch: 182, Validation time: 32.14s \n",
      " Loss: 29.6771: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2772\n",
      "Epoch: 183, Training time: 167.63s, Loss: 29.6677\n",
      "Evaluate current model:\n",
      " Epoch: 183, Validation time: 32.12s \n",
      " Loss: 29.6677: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2761\n",
      "Epoch: 184, Training time: 167.96s, Loss: 29.2524\n",
      "Evaluate current model:\n",
      " Epoch: 184, Validation time: 31.89s \n",
      " Loss: 29.2524: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2774\n",
      "Epoch: 185, Training time: 168.36s, Loss: 29.2693\n",
      "Evaluate current model:\n",
      " Epoch: 185, Validation time: 32.45s \n",
      " Loss: 29.2693: \n",
      " Recall@20: 0.1323 \n",
      " NDCG@20: 0.2764\n",
      "Epoch: 186, Training time: 170.69s, Loss: 29.0713\n",
      "Evaluate current model:\n",
      " Epoch: 186, Validation time: 32.75s \n",
      " Loss: 29.0713: \n",
      " Recall@20: 0.1323 \n",
      " NDCG@20: 0.2775\n",
      "Epoch: 187, Training time: 170.54s, Loss: 28.7624\n",
      "Evaluate current model:\n",
      " Epoch: 187, Validation time: 32.66s \n",
      " Loss: 28.7624: \n",
      " Recall@20: 0.1322 \n",
      " NDCG@20: 0.2766\n",
      "Epoch: 188, Training time: 170.58s, Loss: 28.5863\n",
      "Evaluate current model:\n",
      " Epoch: 188, Validation time: 32.53s \n",
      " Loss: 28.5863: \n",
      " Recall@20: 0.1328 \n",
      " NDCG@20: 0.2777\n",
      "Epoch: 189, Training time: 174.61s, Loss: 28.5440\n",
      "Evaluate current model:\n",
      " Epoch: 189, Validation time: 32.65s \n",
      " Loss: 28.5440: \n",
      " Recall@20: 0.1328 \n",
      " NDCG@20: 0.2789\n",
      "Epoch: 190, Training time: 170.35s, Loss: 28.6249\n",
      "Evaluate current model:\n",
      " Epoch: 190, Validation time: 32.51s \n",
      " Loss: 28.6249: \n",
      " Recall@20: 0.1326 \n",
      " NDCG@20: 0.2783\n",
      "Epoch: 191, Training time: 170.62s, Loss: 28.2165\n",
      "Evaluate current model:\n",
      " Epoch: 191, Validation time: 32.51s \n",
      " Loss: 28.2165: \n",
      " Recall@20: 0.1325 \n",
      " NDCG@20: 0.2773\n",
      "Epoch: 192, Training time: 170.76s, Loss: 28.0575\n",
      "Evaluate current model:\n",
      " Epoch: 192, Validation time: 32.59s \n",
      " Loss: 28.0575: \n",
      " Recall@20: 0.1332 \n",
      " NDCG@20: 0.2788\n",
      "Epoch: 193, Training time: 173.11s, Loss: 27.8507\n",
      "Evaluate current model:\n",
      " Epoch: 193, Validation time: 33.56s \n",
      " Loss: 27.8507: \n",
      " Recall@20: 0.1334 \n",
      " NDCG@20: 0.2787\n",
      "Epoch: 194, Training time: 175.45s, Loss: 27.6432\n",
      "Evaluate current model:\n",
      " Epoch: 194, Validation time: 37.60s \n",
      " Loss: 27.6432: \n",
      " Recall@20: 0.1330 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 195, Training time: 177.81s, Loss: 28.0395\n",
      "Evaluate current model:\n",
      " Epoch: 195, Validation time: 36.43s \n",
      " Loss: 28.0395: \n",
      " Recall@20: 0.1338 \n",
      " NDCG@20: 0.2791\n",
      "Epoch: 196, Training time: 186.40s, Loss: 27.5226\n",
      "Evaluate current model:\n",
      " Epoch: 196, Validation time: 35.59s \n",
      " Loss: 27.5226: \n",
      " Recall@20: 0.1337 \n",
      " NDCG@20: 0.2790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197, Training time: 187.37s, Loss: 27.3666\n",
      "Evaluate current model:\n",
      " Epoch: 197, Validation time: 35.62s \n",
      " Loss: 27.3666: \n",
      " Recall@20: 0.1338 \n",
      " NDCG@20: 0.2794\n",
      "Epoch: 198, Training time: 185.55s, Loss: 27.2534\n",
      "Evaluate current model:\n",
      " Epoch: 198, Validation time: 35.59s \n",
      " Loss: 27.2534: \n",
      " Recall@20: 0.1338 \n",
      " NDCG@20: 0.2789\n",
      "Epoch: 199, Training time: 185.49s, Loss: 27.1449\n",
      "Evaluate current model:\n",
      " Epoch: 199, Validation time: 35.40s \n",
      " Loss: 27.1449: \n",
      " Recall@20: 0.1336 \n",
      " NDCG@20: 0.2797\n",
      "Epoch: 200, Training time: 185.67s, Loss: 26.6577\n",
      "Evaluate current model:\n",
      " Epoch: 200, Validation time: 35.70s \n",
      " Loss: 26.6577: \n",
      " Recall@20: 0.1338 \n",
      " NDCG@20: 0.2797\n",
      "Early stopping at step: 5 log:0.13383880257606506\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from ngcf import NGCF\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "\n",
    "# generate the NGCF-adjacency matrix\n",
    "data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\n",
    "adj_mtx = data_generator.get_adj_mat()\n",
    "\n",
    "# create model name and save\n",
    "modelname =  \"NGCF\" + \\\n",
    "    \"_bs_\" + str(batch_size) + \\\n",
    "    \"_nemb_\" + str(emb_dim) + \\\n",
    "    \"_layers_\" + str(layers) + \\\n",
    "    \"_nodedr_\" + str(node_dropout) + \\\n",
    "    \"_messdr_\" + str(mess_dropout) + \\\n",
    "    \"_reg_\" + str(reg) + \\\n",
    "    \"_lr_\"  + str(lr)\n",
    "\n",
    "# create NGCF model\n",
    "model = NGCF(data_generator.n_users, \n",
    "              data_generator.n_items,\n",
    "              emb_dim,\n",
    "              layers,\n",
    "              reg,\n",
    "              node_dropout,\n",
    "              mess_dropout,\n",
    "              adj_mtx)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# current best metric\n",
    "cur_best_metric = 0\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Set values for early stopping\n",
    "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
    "today = datetime.now()\n",
    "\n",
    "print(\"Start at \" + str(today))\n",
    "print(\"Using \" + str(device) + \" for computations\")\n",
    "print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\n",
    "\n",
    "results = {\"Epoch\": [],\n",
    "            \"Loss\": [],\n",
    "            \"Recall\": [],\n",
    "            \"NDCG\": [],\n",
    "            \"Training Time\": []}\n",
    "\n",
    "for epoch in range(argsn_epochs):\n",
    "\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    training_time = time()-t1\n",
    "    print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\n",
    "        format(epoch, training_time, loss))\n",
    "\n",
    "    # print test evaluation metrics every N epochs (provided by args.eval_N)\n",
    "    if epoch % argseval_N  == (argseval_N - 1):\n",
    "        with torch.no_grad():\n",
    "            t2 = time()\n",
    "            recall, ndcg = eval_model(model.u_g_embeddings.detach(),\n",
    "                                      model.i_g_embeddings.detach(),\n",
    "                                      data_generator.R_train,\n",
    "                                      data_generator.R_test,\n",
    "                                      k)\n",
    "        print(\n",
    "            \"Evaluate current model:\\n\",\n",
    "            \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Loss: {:.4f}:\".format(loss), \"\\n\",\n",
    "            \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\n",
    "            \"NDCG@{}: {:.4f}\".format(k, ndcg)\n",
    "            )\n",
    "\n",
    "        cur_best_metric, stopping_step, should_stop = \\\n",
    "        early_stopping(recall, cur_best_metric, stopping_step, flag_step=5)\n",
    "\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(recall.item())\n",
    "        results['NDCG'].append(ndcg.item())\n",
    "        results['Training Time'].append(training_time)\n",
    "    else:\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(None)\n",
    "        results['NDCG'].append(None)\n",
    "        results['Training Time'].append(training_time)\n",
    "\n",
    "    if should_stop == True: break\n",
    "\n",
    "# save\n",
    "if argssave_results:\n",
    "    date = today.strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "    # save model as .pt file\n",
    "    if os.path.isdir(\"./models\"):\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "    else:\n",
    "        os.mkdir(\"./models\")\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "\n",
    "    # save results as pandas dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.set_index('Epoch', inplace=True)\n",
    "    if os.path.isdir(\"./results\"):\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    else:\n",
    "        os.mkdir(\"./results\")\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    # plot loss\n",
    "    results_df['Loss'].plot(figsize=(12,8), title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNVSdxlSHO7dUm2QfTuNc0g",
   "collapsed_sections": [],
   "name": "NGCFPytorchADV.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
