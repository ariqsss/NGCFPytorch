{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3214,
     "status": "ok",
     "timestamp": 1653631552178,
     "user": {
      "displayName": "Ariq Sudibyo",
      "userId": "10795734971894492673"
     },
     "user_tz": -420
    },
    "id": "l0gLLS0jFQtS",
    "outputId": "9f42b92d-85d3-4efb-a035-00fca2b25bd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from time import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#from utils.load_data import Data\n",
    "#from utils.parser import parse_args\n",
    "#from utils.helper_functions import early_stopping,\\\n",
    "                                   #train,\\\n",
    "                                   #split_matrix,\\\n",
    "                                   #compute_ndcg_k,\\\n",
    "                                   #eval_model\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#data_dir = '/content/drive/My Drive/Colab Notebooks/NGCFPytorch/data/'\n",
    "data_dir = './data/'\n",
    "dataset = 'Gowella'\n",
    "batch_size = 1024\n",
    "layers = eval('[64,64]')\n",
    "emb_dim = 64\n",
    "lr = 0.0001\n",
    "reg = 1e-5\n",
    "mess_dropout = 0.1\n",
    "node_dropout = 0.\n",
    "k = 20\n",
    "argssave_results = 1\n",
    "argsn_epochs = 400\n",
    "argseval_N = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZPqceEnh-ca"
   },
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mlGW8nb1FRpA"
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        # search train_file for max user_id/item_id\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    # first element is the user_id, rest are items\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    # item/user with highest number is number of items/users\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    # number of interactions\n",
    "                    self.n_train += len(items)\n",
    "\n",
    "        # search test_file for max item_id\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if not items:\n",
    "                        print(\"empyt test exists\")\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.n_items = max(self.n_items, max(items))\n",
    "                        self.n_test += len(items)\n",
    "        # adjust counters: user_id/item_id starts at 0\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        # create interactions/ratings matrix 'R' # dok = dictionary of keys\n",
    "        print('Creating interaction matrices R_train and R_test...')\n",
    "        t1 = time()\n",
    "        self.R_train = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32) \n",
    "        self.R_test = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        with open(train_file) as f_train:\n",
    "            with open(test_file) as f_test:\n",
    "                for l in f_train.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    items = [int(i) for i in l.split(' ')]\n",
    "                    uid, train_items = items[0], items[1:]\n",
    "                    # enter 1 if user interacted with item\n",
    "                    for i in train_items:\n",
    "                        self.R_train[uid, i] = 1.\n",
    "                    self.train_items[uid] = train_items\n",
    "\n",
    "                for l in f_test.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    uid, test_items = items[0], items[1:]\n",
    "                    for i in test_items:\n",
    "                        self.R_test[uid, i] = 1.0\n",
    "                    self.test_set[uid] = test_items\n",
    "        print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')\n",
    "\n",
    "    # if exist, get adjacency matrix\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
    "\n",
    "        except Exception:\n",
    "            print('Creating adjacency-matrix...')\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "        return adj_mat\n",
    "    \n",
    "    # create adjancency matrix\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        \n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R_train.tolil() # to list of lists\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        # normalize adjacency matrix\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -.5).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        print('Transforming adjacency-matrix to NGCF-adjacency matrix...')\n",
    "        ngcf_adj_mat = normalized_adj_single(adj_mat) + sp.eye(adj_mat.shape[0])\n",
    "\n",
    "        print('Complete. Transformed adjacency-matrix to NGCF-adjacency matrix in', time() - t2, 'sec.')\n",
    "        return ngcf_adj_mat.tocsr()\n",
    "\n",
    "    # create collections of N items that users never interacted with\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    # sample data for mini-batches\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-0RqzQFiA_l"
   },
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fHPt_8f5FRr_"
   },
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, flag_step, expected_order='asc'):\n",
    "    \"\"\"\n",
    "    Check if early_stopping is needed\n",
    "    Function copied from original code\n",
    "    \"\"\"\n",
    "    assert expected_order in ['asc', 'des']\n",
    "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'des' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= flag_step:\n",
    "        print(\"Early stopping at step: {} log:{}\".format(flag_step, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "\n",
    "    return best_value, stopping_step, should_stop\n",
    "\n",
    "def train(model, data_generator, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model PyTorch style\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    model: PyTorch model\n",
    "    data_generator: Data object\n",
    "    optimizer: PyTorch optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
    "    running_loss=0\n",
    "    for _ in range(n_batch):\n",
    "        u, i, j = data_generator.sample()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u,i,j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss\n",
    "\n",
    "def split_matrix(X, n_splits=100):\n",
    "    \"\"\"\n",
    "    Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    X: matrix to be split\n",
    "    n_folds: number of folds\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    splits: split matrices\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    pred_items: binary tensor with 1s in those locations corresponding to the predicted item interactions\n",
    "    test_items: binary tensor with 1s in locations corresponding to the real test interactions\n",
    "    test_indices: tensor with the location of the top-k predicted items\n",
    "    k: k'th-order \n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    NDCG@k\n",
    "    \"\"\"\n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
    "    dcg = (r[:, :k]/f).sum(1)\n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
    "    ndcg = dcg/dcg_max\n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def eval_model(u_emb, i_emb, Rtr, Rte, k):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    u_emb: User embeddings\n",
    "    i_emb: Item embeddings\n",
    "    Rtr: Sparse matrix with the training interactions\n",
    "    Rte: Sparse matrix with the testing interactions\n",
    "    k : kth-order for metrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result: Dictionary with lists correponding to the metrics at order k for k in Ks\n",
    "    \"\"\"\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        #pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
    "        pred_items.scatter_(dim=1, index=test_indices,src=torch.ones_like(test_indices).float().cuda())\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        #topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices[:, :k]).float().cuda())\n",
    "\n",
    "        TP = (test_items * topk_preds).sum(1)\n",
    "        rec = TP/test_items.sum(1)\n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(recall_k).mean(), torch.cat(ndcg_k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD1RU7SBiCe9"
   },
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JPHUDHUSDOz_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout,\n",
    "        adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.adj_mtx = adj_mtx\n",
    "        self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\n",
    "        self.reg = reg\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.node_dropout = node_dropout\n",
    "        self.mess_dropout = mess_dropout\n",
    "\n",
    "        #self.u_g_embeddings = nn.Parameter(torch.empty(n_users, emb_dim+np.sum(self.layers)))\n",
    "        #self.i_g_embeddings = nn.Parameter(torch.empty(n_items, emb_dim+np.sum(self.layers)))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "        # Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\n",
    "        self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\n",
    "        self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        weight_size_list = [self.emb_dim] + self.layers\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            weight_dict['W_gc_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_gc_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "            \n",
    "            weight_dict['W_bi_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_bi_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        \n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\n",
    "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
    "\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        # forward pass for 'n' propagation layers\n",
    "        for k in range(self.n_layers):\n",
    "\n",
    "            # weighted sum messages of neighbours\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum weighted sum messages of neighbours\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbours\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\n",
    "            # transformed bi messages of neighbours\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation \n",
    "            ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
    "            # + message dropout\n",
    "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
    "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
    "\n",
    "            # normalize activation\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings.append(norm_embeddings)\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        \n",
    "        # back to user/item dimension\n",
    "        u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_g_embeddings = nn.Parameter(u_g_embeddings)\n",
    "        self.i_g_embeddings = nn.Parameter(i_g_embeddings)\n",
    "        \n",
    "        u_emb = u_g_embeddings[u] # user embeddings\n",
    "        p_emb = i_g_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_g_embeddings[j] # negative item embeddings\n",
    "\n",
    "        y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\n",
    "        y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\n",
    "        log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
    "\n",
    "        # compute bpr-loss\n",
    "        bpr_loss = -log_prob\n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg  = self.reg*l2norm\n",
    "            bpr_loss =  -log_prob + l2reg\n",
    "\n",
    "        return bpr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0VfBH1ZiD7Q"
   },
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMy8xzwPESNw",
    "outputId": "8eafd7e9-09a3-471e-b2d8-64c81cc86677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n",
      "Creating interaction matrices R_train and R_test...\n",
      "Complete. Interaction matrices R_train and R_test created in 10.930862665176392 sec\n",
      "Loaded adjacency-matrix (shape: (70839, 70839) ) in 0.11716198921203613 sec.\n",
      "Initializing weights...\n",
      "Weights initialized.\n",
      "Start at 2022-05-30 21:43:04.701023\n",
      "Using cuda for computations\n",
      "Params on CUDA: True\n",
      "Epoch: 0, Training time: 160.38s, Loss: 510.9547\n",
      "Evaluate current model:\n",
      " Epoch: 0, Validation time: 38.63s \n",
      " Loss: 510.9547: \n",
      " Recall@20: 0.0017 \n",
      " NDCG@20: 0.0062\n",
      "Epoch: 1, Training time: 159.42s, Loss: 377.6990\n",
      "Evaluate current model:\n",
      " Epoch: 1, Validation time: 39.22s \n",
      " Loss: 377.6990: \n",
      " Recall@20: 0.0040 \n",
      " NDCG@20: 0.0115\n",
      "Epoch: 2, Training time: 161.90s, Loss: 310.0618\n",
      "Evaluate current model:\n",
      " Epoch: 2, Validation time: 42.09s \n",
      " Loss: 310.0618: \n",
      " Recall@20: 0.0052 \n",
      " NDCG@20: 0.0139\n",
      "Epoch: 3, Training time: 164.73s, Loss: 284.7331\n",
      "Evaluate current model:\n",
      " Epoch: 3, Validation time: 39.34s \n",
      " Loss: 284.7331: \n",
      " Recall@20: 0.0067 \n",
      " NDCG@20: 0.0173\n",
      "Epoch: 4, Training time: 166.44s, Loss: 268.5869\n",
      "Evaluate current model:\n",
      " Epoch: 4, Validation time: 38.84s \n",
      " Loss: 268.5869: \n",
      " Recall@20: 0.0102 \n",
      " NDCG@20: 0.0257\n",
      "Epoch: 5, Training time: 163.65s, Loss: 254.2155\n",
      "Evaluate current model:\n",
      " Epoch: 5, Validation time: 36.84s \n",
      " Loss: 254.2155: \n",
      " Recall@20: 0.0144 \n",
      " NDCG@20: 0.0331\n",
      "Epoch: 6, Training time: 159.71s, Loss: 238.0882\n",
      "Evaluate current model:\n",
      " Epoch: 6, Validation time: 34.53s \n",
      " Loss: 238.0882: \n",
      " Recall@20: 0.0238 \n",
      " NDCG@20: 0.0561\n",
      "Epoch: 7, Training time: 159.73s, Loss: 225.5212\n",
      "Evaluate current model:\n",
      " Epoch: 7, Validation time: 33.11s \n",
      " Loss: 225.5212: \n",
      " Recall@20: 0.0219 \n",
      " NDCG@20: 0.0506\n",
      "Epoch: 8, Training time: 159.44s, Loss: 216.5708\n",
      "Evaluate current model:\n",
      " Epoch: 8, Validation time: 34.27s \n",
      " Loss: 216.5708: \n",
      " Recall@20: 0.0285 \n",
      " NDCG@20: 0.0662\n",
      "Epoch: 9, Training time: 165.98s, Loss: 210.7262\n",
      "Evaluate current model:\n",
      " Epoch: 9, Validation time: 36.26s \n",
      " Loss: 210.7262: \n",
      " Recall@20: 0.0285 \n",
      " NDCG@20: 0.0665\n",
      "Epoch: 10, Training time: 166.32s, Loss: 206.2764\n",
      "Evaluate current model:\n",
      " Epoch: 10, Validation time: 35.39s \n",
      " Loss: 206.2764: \n",
      " Recall@20: 0.0319 \n",
      " NDCG@20: 0.0718\n",
      "Epoch: 11, Training time: 166.69s, Loss: 201.8828\n",
      "Evaluate current model:\n",
      " Epoch: 11, Validation time: 35.53s \n",
      " Loss: 201.8828: \n",
      " Recall@20: 0.0345 \n",
      " NDCG@20: 0.0759\n",
      "Epoch: 12, Training time: 166.40s, Loss: 198.5265\n",
      "Evaluate current model:\n",
      " Epoch: 12, Validation time: 35.90s \n",
      " Loss: 198.5265: \n",
      " Recall@20: 0.0374 \n",
      " NDCG@20: 0.0824\n",
      "Epoch: 13, Training time: 166.89s, Loss: 194.8195\n",
      "Evaluate current model:\n",
      " Epoch: 13, Validation time: 35.55s \n",
      " Loss: 194.8195: \n",
      " Recall@20: 0.0412 \n",
      " NDCG@20: 0.0886\n",
      "Epoch: 14, Training time: 166.66s, Loss: 191.6820\n",
      "Evaluate current model:\n",
      " Epoch: 14, Validation time: 35.62s \n",
      " Loss: 191.6820: \n",
      " Recall@20: 0.0422 \n",
      " NDCG@20: 0.0879\n",
      "Epoch: 15, Training time: 167.10s, Loss: 189.1505\n",
      "Evaluate current model:\n",
      " Epoch: 15, Validation time: 36.97s \n",
      " Loss: 189.1505: \n",
      " Recall@20: 0.0429 \n",
      " NDCG@20: 0.0912\n",
      "Epoch: 16, Training time: 167.16s, Loss: 186.4603\n",
      "Evaluate current model:\n",
      " Epoch: 16, Validation time: 35.98s \n",
      " Loss: 186.4603: \n",
      " Recall@20: 0.0446 \n",
      " NDCG@20: 0.0965\n",
      "Epoch: 17, Training time: 166.74s, Loss: 184.2661\n",
      "Evaluate current model:\n",
      " Epoch: 17, Validation time: 36.01s \n",
      " Loss: 184.2661: \n",
      " Recall@20: 0.0469 \n",
      " NDCG@20: 0.0987\n",
      "Epoch: 18, Training time: 167.11s, Loss: 182.2255\n",
      "Evaluate current model:\n",
      " Epoch: 18, Validation time: 35.96s \n",
      " Loss: 182.2255: \n",
      " Recall@20: 0.0490 \n",
      " NDCG@20: 0.1026\n",
      "Epoch: 19, Training time: 167.23s, Loss: 180.8687\n",
      "Evaluate current model:\n",
      " Epoch: 19, Validation time: 36.73s \n",
      " Loss: 180.8687: \n",
      " Recall@20: 0.0502 \n",
      " NDCG@20: 0.1060\n",
      "Epoch: 20, Training time: 167.18s, Loss: 178.3614\n",
      "Evaluate current model:\n",
      " Epoch: 20, Validation time: 35.73s \n",
      " Loss: 178.3614: \n",
      " Recall@20: 0.0516 \n",
      " NDCG@20: 0.1100\n",
      "Epoch: 21, Training time: 167.24s, Loss: 176.5981\n",
      "Evaluate current model:\n",
      " Epoch: 21, Validation time: 35.41s \n",
      " Loss: 176.5981: \n",
      " Recall@20: 0.0544 \n",
      " NDCG@20: 0.1139\n",
      "Epoch: 22, Training time: 166.76s, Loss: 175.4474\n",
      "Evaluate current model:\n",
      " Epoch: 22, Validation time: 36.18s \n",
      " Loss: 175.4474: \n",
      " Recall@20: 0.0542 \n",
      " NDCG@20: 0.1166\n",
      "Epoch: 23, Training time: 166.56s, Loss: 173.5788\n",
      "Evaluate current model:\n",
      " Epoch: 23, Validation time: 35.82s \n",
      " Loss: 173.5788: \n",
      " Recall@20: 0.0551 \n",
      " NDCG@20: 0.1182\n",
      "Epoch: 24, Training time: 166.34s, Loss: 172.0837\n",
      "Evaluate current model:\n",
      " Epoch: 24, Validation time: 35.46s \n",
      " Loss: 172.0837: \n",
      " Recall@20: 0.0556 \n",
      " NDCG@20: 0.1189\n",
      "Epoch: 25, Training time: 166.70s, Loss: 170.7340\n",
      "Evaluate current model:\n",
      " Epoch: 25, Validation time: 35.41s \n",
      " Loss: 170.7340: \n",
      " Recall@20: 0.0579 \n",
      " NDCG@20: 0.1213\n",
      "Epoch: 26, Training time: 166.26s, Loss: 169.0800\n",
      "Evaluate current model:\n",
      " Epoch: 26, Validation time: 35.20s \n",
      " Loss: 169.0800: \n",
      " Recall@20: 0.0590 \n",
      " NDCG@20: 0.1266\n",
      "Epoch: 27, Training time: 159.32s, Loss: 167.3339\n",
      "Evaluate current model:\n",
      " Epoch: 27, Validation time: 32.64s \n",
      " Loss: 167.3339: \n",
      " Recall@20: 0.0598 \n",
      " NDCG@20: 0.1279\n",
      "Epoch: 28, Training time: 158.95s, Loss: 165.9114\n",
      "Evaluate current model:\n",
      " Epoch: 28, Validation time: 32.64s \n",
      " Loss: 165.9114: \n",
      " Recall@20: 0.0615 \n",
      " NDCG@20: 0.1323\n",
      "Epoch: 29, Training time: 158.92s, Loss: 164.5278\n",
      "Evaluate current model:\n",
      " Epoch: 29, Validation time: 32.60s \n",
      " Loss: 164.5278: \n",
      " Recall@20: 0.0640 \n",
      " NDCG@20: 0.1371\n",
      "Epoch: 30, Training time: 158.91s, Loss: 163.1214\n",
      "Evaluate current model:\n",
      " Epoch: 30, Validation time: 32.66s \n",
      " Loss: 163.1214: \n",
      " Recall@20: 0.0626 \n",
      " NDCG@20: 0.1364\n",
      "Epoch: 31, Training time: 158.89s, Loss: 161.9165\n",
      "Evaluate current model:\n",
      " Epoch: 31, Validation time: 32.76s \n",
      " Loss: 161.9165: \n",
      " Recall@20: 0.0649 \n",
      " NDCG@20: 0.1391\n",
      "Epoch: 32, Training time: 158.98s, Loss: 160.4108\n",
      "Evaluate current model:\n",
      " Epoch: 32, Validation time: 32.85s \n",
      " Loss: 160.4108: \n",
      " Recall@20: 0.0676 \n",
      " NDCG@20: 0.1450\n",
      "Epoch: 33, Training time: 158.88s, Loss: 158.7376\n",
      "Evaluate current model:\n",
      " Epoch: 33, Validation time: 32.68s \n",
      " Loss: 158.7376: \n",
      " Recall@20: 0.0690 \n",
      " NDCG@20: 0.1477\n",
      "Epoch: 34, Training time: 158.91s, Loss: 157.0420\n",
      "Evaluate current model:\n",
      " Epoch: 34, Validation time: 32.82s \n",
      " Loss: 157.0420: \n",
      " Recall@20: 0.0696 \n",
      " NDCG@20: 0.1478\n",
      "Epoch: 35, Training time: 158.92s, Loss: 155.3284\n",
      "Evaluate current model:\n",
      " Epoch: 35, Validation time: 32.63s \n",
      " Loss: 155.3284: \n",
      " Recall@20: 0.0726 \n",
      " NDCG@20: 0.1584\n",
      "Epoch: 36, Training time: 158.96s, Loss: 153.9026\n",
      "Evaluate current model:\n",
      " Epoch: 36, Validation time: 32.64s \n",
      " Loss: 153.9026: \n",
      " Recall@20: 0.0725 \n",
      " NDCG@20: 0.1566\n",
      "Epoch: 37, Training time: 158.95s, Loss: 152.3167\n",
      "Evaluate current model:\n",
      " Epoch: 37, Validation time: 32.66s \n",
      " Loss: 152.3167: \n",
      " Recall@20: 0.0744 \n",
      " NDCG@20: 0.1613\n",
      "Epoch: 38, Training time: 158.78s, Loss: 150.6010\n",
      "Evaluate current model:\n",
      " Epoch: 38, Validation time: 32.66s \n",
      " Loss: 150.6010: \n",
      " Recall@20: 0.0762 \n",
      " NDCG@20: 0.1631\n",
      "Epoch: 39, Training time: 158.92s, Loss: 148.9231\n",
      "Evaluate current model:\n",
      " Epoch: 39, Validation time: 32.63s \n",
      " Loss: 148.9231: \n",
      " Recall@20: 0.0779 \n",
      " NDCG@20: 0.1684\n",
      "Epoch: 40, Training time: 158.95s, Loss: 147.7234\n",
      "Evaluate current model:\n",
      " Epoch: 40, Validation time: 32.81s \n",
      " Loss: 147.7234: \n",
      " Recall@20: 0.0791 \n",
      " NDCG@20: 0.1711\n",
      "Epoch: 41, Training time: 158.88s, Loss: 145.8553\n",
      "Evaluate current model:\n",
      " Epoch: 41, Validation time: 32.85s \n",
      " Loss: 145.8553: \n",
      " Recall@20: 0.0812 \n",
      " NDCG@20: 0.1779\n",
      "Epoch: 42, Training time: 158.94s, Loss: 144.3012\n",
      "Evaluate current model:\n",
      " Epoch: 42, Validation time: 32.57s \n",
      " Loss: 144.3012: \n",
      " Recall@20: 0.0830 \n",
      " NDCG@20: 0.1822\n",
      "Epoch: 43, Training time: 158.91s, Loss: 142.2708\n",
      "Evaluate current model:\n",
      " Epoch: 43, Validation time: 32.85s \n",
      " Loss: 142.2708: \n",
      " Recall@20: 0.0844 \n",
      " NDCG@20: 0.1842\n",
      "Epoch: 44, Training time: 158.98s, Loss: 140.9738\n",
      "Evaluate current model:\n",
      " Epoch: 44, Validation time: 32.85s \n",
      " Loss: 140.9738: \n",
      " Recall@20: 0.0860 \n",
      " NDCG@20: 0.1881\n",
      "Epoch: 45, Training time: 158.95s, Loss: 139.1706\n",
      "Evaluate current model:\n",
      " Epoch: 45, Validation time: 32.82s \n",
      " Loss: 139.1706: \n",
      " Recall@20: 0.0881 \n",
      " NDCG@20: 0.1922\n",
      "Epoch: 46, Training time: 158.86s, Loss: 137.2254\n",
      "Evaluate current model:\n",
      " Epoch: 46, Validation time: 33.01s \n",
      " Loss: 137.2254: \n",
      " Recall@20: 0.0893 \n",
      " NDCG@20: 0.1962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Training time: 158.94s, Loss: 135.5650\n",
      "Evaluate current model:\n",
      " Epoch: 47, Validation time: 32.72s \n",
      " Loss: 135.5650: \n",
      " Recall@20: 0.0905 \n",
      " NDCG@20: 0.1980\n",
      "Epoch: 48, Training time: 158.88s, Loss: 133.7869\n",
      "Evaluate current model:\n",
      " Epoch: 48, Validation time: 32.72s \n",
      " Loss: 133.7869: \n",
      " Recall@20: 0.0913 \n",
      " NDCG@20: 0.2003\n",
      "Epoch: 49, Training time: 158.97s, Loss: 131.6200\n",
      "Evaluate current model:\n",
      " Epoch: 49, Validation time: 32.73s \n",
      " Loss: 131.6200: \n",
      " Recall@20: 0.0942 \n",
      " NDCG@20: 0.2041\n",
      "Epoch: 50, Training time: 159.00s, Loss: 130.0421\n",
      "Evaluate current model:\n",
      " Epoch: 50, Validation time: 32.73s \n",
      " Loss: 130.0421: \n",
      " Recall@20: 0.0957 \n",
      " NDCG@20: 0.2106\n",
      "Epoch: 51, Training time: 158.90s, Loss: 128.5015\n",
      "Evaluate current model:\n",
      " Epoch: 51, Validation time: 32.84s \n",
      " Loss: 128.5015: \n",
      " Recall@20: 0.0961 \n",
      " NDCG@20: 0.2115\n",
      "Epoch: 52, Training time: 158.87s, Loss: 126.5366\n",
      "Evaluate current model:\n",
      " Epoch: 52, Validation time: 32.83s \n",
      " Loss: 126.5366: \n",
      " Recall@20: 0.0979 \n",
      " NDCG@20: 0.2135\n",
      "Epoch: 53, Training time: 158.95s, Loss: 125.0358\n",
      "Evaluate current model:\n",
      " Epoch: 53, Validation time: 32.80s \n",
      " Loss: 125.0358: \n",
      " Recall@20: 0.0989 \n",
      " NDCG@20: 0.2182\n",
      "Epoch: 54, Training time: 158.84s, Loss: 123.3096\n",
      "Evaluate current model:\n",
      " Epoch: 54, Validation time: 32.64s \n",
      " Loss: 123.3096: \n",
      " Recall@20: 0.0988 \n",
      " NDCG@20: 0.2182\n",
      "Epoch: 55, Training time: 158.97s, Loss: 121.6435\n",
      "Evaluate current model:\n",
      " Epoch: 55, Validation time: 32.67s \n",
      " Loss: 121.6435: \n",
      " Recall@20: 0.1004 \n",
      " NDCG@20: 0.2221\n",
      "Epoch: 56, Training time: 158.92s, Loss: 119.5662\n",
      "Evaluate current model:\n",
      " Epoch: 56, Validation time: 32.72s \n",
      " Loss: 119.5662: \n",
      " Recall@20: 0.1007 \n",
      " NDCG@20: 0.2208\n",
      "Epoch: 57, Training time: 158.93s, Loss: 117.8546\n",
      "Evaluate current model:\n",
      " Epoch: 57, Validation time: 32.73s \n",
      " Loss: 117.8546: \n",
      " Recall@20: 0.1020 \n",
      " NDCG@20: 0.2237\n",
      "Epoch: 58, Training time: 158.86s, Loss: 116.2077\n",
      "Evaluate current model:\n",
      " Epoch: 58, Validation time: 32.70s \n",
      " Loss: 116.2077: \n",
      " Recall@20: 0.1030 \n",
      " NDCG@20: 0.2263\n",
      "Epoch: 59, Training time: 158.97s, Loss: 114.4958\n",
      "Evaluate current model:\n",
      " Epoch: 59, Validation time: 33.06s \n",
      " Loss: 114.4958: \n",
      " Recall@20: 0.1035 \n",
      " NDCG@20: 0.2285\n",
      "Epoch: 60, Training time: 158.94s, Loss: 112.7010\n",
      "Evaluate current model:\n",
      " Epoch: 60, Validation time: 32.84s \n",
      " Loss: 112.7010: \n",
      " Recall@20: 0.1042 \n",
      " NDCG@20: 0.2307\n",
      "Epoch: 61, Training time: 158.81s, Loss: 111.3320\n",
      "Evaluate current model:\n",
      " Epoch: 61, Validation time: 32.76s \n",
      " Loss: 111.3320: \n",
      " Recall@20: 0.1048 \n",
      " NDCG@20: 0.2310\n",
      "Epoch: 62, Training time: 158.96s, Loss: 109.2943\n",
      "Evaluate current model:\n",
      " Epoch: 62, Validation time: 32.72s \n",
      " Loss: 109.2943: \n",
      " Recall@20: 0.1048 \n",
      " NDCG@20: 0.2319\n",
      "Epoch: 63, Training time: 158.92s, Loss: 107.5792\n",
      "Evaluate current model:\n",
      " Epoch: 63, Validation time: 32.75s \n",
      " Loss: 107.5792: \n",
      " Recall@20: 0.1059 \n",
      " NDCG@20: 0.2337\n",
      "Epoch: 64, Training time: 158.82s, Loss: 106.1307\n",
      "Evaluate current model:\n",
      " Epoch: 64, Validation time: 32.65s \n",
      " Loss: 106.1307: \n",
      " Recall@20: 0.1068 \n",
      " NDCG@20: 0.2348\n",
      "Epoch: 65, Training time: 158.88s, Loss: 104.3470\n",
      "Evaluate current model:\n",
      " Epoch: 65, Validation time: 32.84s \n",
      " Loss: 104.3470: \n",
      " Recall@20: 0.1073 \n",
      " NDCG@20: 0.2359\n",
      "Epoch: 66, Training time: 158.90s, Loss: 102.8382\n",
      "Evaluate current model:\n",
      " Epoch: 66, Validation time: 32.61s \n",
      " Loss: 102.8382: \n",
      " Recall@20: 0.1077 \n",
      " NDCG@20: 0.2361\n",
      "Epoch: 67, Training time: 158.85s, Loss: 101.3649\n",
      "Evaluate current model:\n",
      " Epoch: 67, Validation time: 32.75s \n",
      " Loss: 101.3649: \n",
      " Recall@20: 0.1080 \n",
      " NDCG@20: 0.2375\n",
      "Epoch: 68, Training time: 158.84s, Loss: 99.9306\n",
      "Evaluate current model:\n",
      " Epoch: 68, Validation time: 32.85s \n",
      " Loss: 99.9306: \n",
      " Recall@20: 0.1087 \n",
      " NDCG@20: 0.2386\n",
      "Epoch: 69, Training time: 158.89s, Loss: 98.3899\n",
      "Evaluate current model:\n",
      " Epoch: 69, Validation time: 32.76s \n",
      " Loss: 98.3899: \n",
      " Recall@20: 0.1088 \n",
      " NDCG@20: 0.2398\n",
      "Epoch: 70, Training time: 158.88s, Loss: 96.4914\n",
      "Evaluate current model:\n",
      " Epoch: 70, Validation time: 32.93s \n",
      " Loss: 96.4914: \n",
      " Recall@20: 0.1098 \n",
      " NDCG@20: 0.2415\n",
      "Epoch: 71, Training time: 158.96s, Loss: 94.7215\n",
      "Evaluate current model:\n",
      " Epoch: 71, Validation time: 33.12s \n",
      " Loss: 94.7215: \n",
      " Recall@20: 0.1107 \n",
      " NDCG@20: 0.2422\n",
      "Epoch: 72, Training time: 158.88s, Loss: 93.8557\n",
      "Evaluate current model:\n",
      " Epoch: 72, Validation time: 33.44s \n",
      " Loss: 93.8557: \n",
      " Recall@20: 0.1107 \n",
      " NDCG@20: 0.2435\n",
      "Epoch: 73, Training time: 158.85s, Loss: 92.5492\n",
      "Evaluate current model:\n",
      " Epoch: 73, Validation time: 32.76s \n",
      " Loss: 92.5492: \n",
      " Recall@20: 0.1111 \n",
      " NDCG@20: 0.2437\n",
      "Epoch: 74, Training time: 158.95s, Loss: 90.6245\n",
      "Evaluate current model:\n",
      " Epoch: 74, Validation time: 32.85s \n",
      " Loss: 90.6245: \n",
      " Recall@20: 0.1114 \n",
      " NDCG@20: 0.2446\n",
      "Epoch: 75, Training time: 158.88s, Loss: 89.8099\n",
      "Evaluate current model:\n",
      " Epoch: 75, Validation time: 32.76s \n",
      " Loss: 89.8099: \n",
      " Recall@20: 0.1119 \n",
      " NDCG@20: 0.2460\n",
      "Epoch: 76, Training time: 158.90s, Loss: 88.1509\n",
      "Evaluate current model:\n",
      " Epoch: 76, Validation time: 32.76s \n",
      " Loss: 88.1509: \n",
      " Recall@20: 0.1124 \n",
      " NDCG@20: 0.2467\n",
      "Epoch: 77, Training time: 158.87s, Loss: 86.7947\n",
      "Evaluate current model:\n",
      " Epoch: 77, Validation time: 33.03s \n",
      " Loss: 86.7947: \n",
      " Recall@20: 0.1130 \n",
      " NDCG@20: 0.2475\n",
      "Epoch: 78, Training time: 158.83s, Loss: 85.6668\n",
      "Evaluate current model:\n",
      " Epoch: 78, Validation time: 32.85s \n",
      " Loss: 85.6668: \n",
      " Recall@20: 0.1136 \n",
      " NDCG@20: 0.2494\n",
      "Epoch: 79, Training time: 158.94s, Loss: 83.8916\n",
      "Evaluate current model:\n",
      " Epoch: 79, Validation time: 33.25s \n",
      " Loss: 83.8916: \n",
      " Recall@20: 0.1145 \n",
      " NDCG@20: 0.2487\n",
      "Epoch: 80, Training time: 158.82s, Loss: 82.6835\n",
      "Evaluate current model:\n",
      " Epoch: 80, Validation time: 33.08s \n",
      " Loss: 82.6835: \n",
      " Recall@20: 0.1146 \n",
      " NDCG@20: 0.2492\n",
      "Epoch: 81, Training time: 158.71s, Loss: 81.9880\n",
      "Evaluate current model:\n",
      " Epoch: 81, Validation time: 33.08s \n",
      " Loss: 81.9880: \n",
      " Recall@20: 0.1153 \n",
      " NDCG@20: 0.2509\n",
      "Epoch: 82, Training time: 158.82s, Loss: 80.8322\n",
      "Evaluate current model:\n",
      " Epoch: 82, Validation time: 32.82s \n",
      " Loss: 80.8322: \n",
      " Recall@20: 0.1154 \n",
      " NDCG@20: 0.2510\n",
      "Epoch: 83, Training time: 158.80s, Loss: 79.0678\n",
      "Evaluate current model:\n",
      " Epoch: 83, Validation time: 32.95s \n",
      " Loss: 79.0678: \n",
      " Recall@20: 0.1156 \n",
      " NDCG@20: 0.2521\n",
      "Epoch: 84, Training time: 158.80s, Loss: 78.3571\n",
      "Evaluate current model:\n",
      " Epoch: 84, Validation time: 33.11s \n",
      " Loss: 78.3571: \n",
      " Recall@20: 0.1166 \n",
      " NDCG@20: 0.2510\n",
      "Epoch: 85, Training time: 158.93s, Loss: 77.1480\n",
      "Evaluate current model:\n",
      " Epoch: 85, Validation time: 32.98s \n",
      " Loss: 77.1480: \n",
      " Recall@20: 0.1161 \n",
      " NDCG@20: 0.2516\n",
      "Epoch: 86, Training time: 158.77s, Loss: 76.1925\n",
      "Evaluate current model:\n",
      " Epoch: 86, Validation time: 33.09s \n",
      " Loss: 76.1925: \n",
      " Recall@20: 0.1161 \n",
      " NDCG@20: 0.2528\n",
      "Epoch: 87, Training time: 158.75s, Loss: 74.5377\n",
      "Evaluate current model:\n",
      " Epoch: 87, Validation time: 33.08s \n",
      " Loss: 74.5377: \n",
      " Recall@20: 0.1174 \n",
      " NDCG@20: 0.2533\n",
      "Epoch: 88, Training time: 158.84s, Loss: 73.6216\n",
      "Evaluate current model:\n",
      " Epoch: 88, Validation time: 32.83s \n",
      " Loss: 73.6216: \n",
      " Recall@20: 0.1174 \n",
      " NDCG@20: 0.2533\n",
      "Epoch: 89, Training time: 158.90s, Loss: 72.9014\n",
      "Evaluate current model:\n",
      " Epoch: 89, Validation time: 32.78s \n",
      " Loss: 72.9014: \n",
      " Recall@20: 0.1178 \n",
      " NDCG@20: 0.2546\n",
      "Epoch: 90, Training time: 158.91s, Loss: 72.0326\n",
      "Evaluate current model:\n",
      " Epoch: 90, Validation time: 32.83s \n",
      " Loss: 72.0326: \n",
      " Recall@20: 0.1181 \n",
      " NDCG@20: 0.2542\n",
      "Epoch: 91, Training time: 158.81s, Loss: 70.6985\n",
      "Evaluate current model:\n",
      " Epoch: 91, Validation time: 32.68s \n",
      " Loss: 70.6985: \n",
      " Recall@20: 0.1181 \n",
      " NDCG@20: 0.2546\n",
      "Epoch: 92, Training time: 158.84s, Loss: 69.7473\n",
      "Evaluate current model:\n",
      " Epoch: 92, Validation time: 32.90s \n",
      " Loss: 69.7473: \n",
      " Recall@20: 0.1178 \n",
      " NDCG@20: 0.2548\n",
      "Epoch: 93, Training time: 158.79s, Loss: 68.7112\n",
      "Evaluate current model:\n",
      " Epoch: 93, Validation time: 32.93s \n",
      " Loss: 68.7112: \n",
      " Recall@20: 0.1182 \n",
      " NDCG@20: 0.2549\n",
      "Epoch: 94, Training time: 158.88s, Loss: 67.9947\n",
      "Evaluate current model:\n",
      " Epoch: 94, Validation time: 32.76s \n",
      " Loss: 67.9947: \n",
      " Recall@20: 0.1187 \n",
      " NDCG@20: 0.2556\n",
      "Epoch: 95, Training time: 158.72s, Loss: 67.1217\n",
      "Evaluate current model:\n",
      " Epoch: 95, Validation time: 33.00s \n",
      " Loss: 67.1217: \n",
      " Recall@20: 0.1191 \n",
      " NDCG@20: 0.2568\n",
      "Epoch: 96, Training time: 158.83s, Loss: 65.9801\n",
      "Evaluate current model:\n",
      " Epoch: 96, Validation time: 33.30s \n",
      " Loss: 65.9801: \n",
      " Recall@20: 0.1191 \n",
      " NDCG@20: 0.2571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Training time: 158.86s, Loss: 65.3655\n",
      "Evaluate current model:\n",
      " Epoch: 97, Validation time: 32.89s \n",
      " Loss: 65.3655: \n",
      " Recall@20: 0.1190 \n",
      " NDCG@20: 0.2577\n",
      "Epoch: 98, Training time: 158.87s, Loss: 64.3630\n",
      "Evaluate current model:\n",
      " Epoch: 98, Validation time: 33.01s \n",
      " Loss: 64.3630: \n",
      " Recall@20: 0.1189 \n",
      " NDCG@20: 0.2577\n",
      "Epoch: 99, Training time: 158.93s, Loss: 63.7236\n",
      "Evaluate current model:\n",
      " Epoch: 99, Validation time: 33.11s \n",
      " Loss: 63.7236: \n",
      " Recall@20: 0.1194 \n",
      " NDCG@20: 0.2583\n",
      "Epoch: 100, Training time: 158.97s, Loss: 62.9060\n",
      "Evaluate current model:\n",
      " Epoch: 100, Validation time: 33.09s \n",
      " Loss: 62.9060: \n",
      " Recall@20: 0.1199 \n",
      " NDCG@20: 0.2575\n",
      "Epoch: 101, Training time: 158.89s, Loss: 61.9855\n",
      "Evaluate current model:\n",
      " Epoch: 101, Validation time: 32.84s \n",
      " Loss: 61.9855: \n",
      " Recall@20: 0.1203 \n",
      " NDCG@20: 0.2584\n",
      "Epoch: 102, Training time: 158.89s, Loss: 60.9862\n",
      "Evaluate current model:\n",
      " Epoch: 102, Validation time: 32.92s \n",
      " Loss: 60.9862: \n",
      " Recall@20: 0.1207 \n",
      " NDCG@20: 0.2582\n",
      "Epoch: 103, Training time: 158.95s, Loss: 60.5659\n",
      "Evaluate current model:\n",
      " Epoch: 103, Validation time: 32.94s \n",
      " Loss: 60.5659: \n",
      " Recall@20: 0.1207 \n",
      " NDCG@20: 0.2592\n",
      "Epoch: 104, Training time: 158.90s, Loss: 59.6810\n",
      "Evaluate current model:\n",
      " Epoch: 104, Validation time: 32.91s \n",
      " Loss: 59.6810: \n",
      " Recall@20: 0.1206 \n",
      " NDCG@20: 0.2592\n",
      "Epoch: 105, Training time: 158.94s, Loss: 58.9810\n",
      "Evaluate current model:\n",
      " Epoch: 105, Validation time: 32.77s \n",
      " Loss: 58.9810: \n",
      " Recall@20: 0.1204 \n",
      " NDCG@20: 0.2594\n",
      "Epoch: 106, Training time: 158.83s, Loss: 58.2732\n",
      "Evaluate current model:\n",
      " Epoch: 106, Validation time: 32.80s \n",
      " Loss: 58.2732: \n",
      " Recall@20: 0.1207 \n",
      " NDCG@20: 0.2592\n",
      "Epoch: 107, Training time: 159.49s, Loss: 57.3889\n",
      "Evaluate current model:\n",
      " Epoch: 107, Validation time: 41.28s \n",
      " Loss: 57.3889: \n",
      " Recall@20: 0.1215 \n",
      " NDCG@20: 0.2599\n",
      "Epoch: 108, Training time: 158.93s, Loss: 56.7054\n",
      "Evaluate current model:\n",
      " Epoch: 108, Validation time: 32.73s \n",
      " Loss: 56.7054: \n",
      " Recall@20: 0.1214 \n",
      " NDCG@20: 0.2606\n",
      "Epoch: 109, Training time: 159.15s, Loss: 56.4664\n",
      "Evaluate current model:\n",
      " Epoch: 109, Validation time: 32.88s \n",
      " Loss: 56.4664: \n",
      " Recall@20: 0.1217 \n",
      " NDCG@20: 0.2608\n",
      "Epoch: 110, Training time: 158.87s, Loss: 55.2836\n",
      "Evaluate current model:\n",
      " Epoch: 110, Validation time: 32.72s \n",
      " Loss: 55.2836: \n",
      " Recall@20: 0.1218 \n",
      " NDCG@20: 0.2610\n",
      "Epoch: 111, Training time: 158.94s, Loss: 54.6339\n",
      "Evaluate current model:\n",
      " Epoch: 111, Validation time: 32.82s \n",
      " Loss: 54.6339: \n",
      " Recall@20: 0.1217 \n",
      " NDCG@20: 0.2609\n",
      "Epoch: 112, Training time: 158.88s, Loss: 53.8286\n",
      "Evaluate current model:\n",
      " Epoch: 112, Validation time: 32.84s \n",
      " Loss: 53.8286: \n",
      " Recall@20: 0.1216 \n",
      " NDCG@20: 0.2607\n",
      "Epoch: 113, Training time: 158.85s, Loss: 53.5641\n",
      "Evaluate current model:\n",
      " Epoch: 113, Validation time: 32.73s \n",
      " Loss: 53.5641: \n",
      " Recall@20: 0.1219 \n",
      " NDCG@20: 0.2609\n",
      "Epoch: 114, Training time: 158.90s, Loss: 52.7600\n",
      "Evaluate current model:\n",
      " Epoch: 114, Validation time: 32.63s \n",
      " Loss: 52.7600: \n",
      " Recall@20: 0.1225 \n",
      " NDCG@20: 0.2618\n",
      "Epoch: 115, Training time: 158.99s, Loss: 52.4935\n",
      "Evaluate current model:\n",
      " Epoch: 115, Validation time: 32.87s \n",
      " Loss: 52.4935: \n",
      " Recall@20: 0.1223 \n",
      " NDCG@20: 0.2609\n",
      "Epoch: 116, Training time: 158.93s, Loss: 51.6785\n",
      "Evaluate current model:\n",
      " Epoch: 116, Validation time: 33.05s \n",
      " Loss: 51.6785: \n",
      " Recall@20: 0.1225 \n",
      " NDCG@20: 0.2613\n",
      "Epoch: 117, Training time: 159.83s, Loss: 51.1964\n",
      "Evaluate current model:\n",
      " Epoch: 117, Validation time: 33.53s \n",
      " Loss: 51.1964: \n",
      " Recall@20: 0.1232 \n",
      " NDCG@20: 0.2626\n",
      "Epoch: 118, Training time: 159.97s, Loss: 50.5868\n",
      "Evaluate current model:\n",
      " Epoch: 118, Validation time: 33.88s \n",
      " Loss: 50.5868: \n",
      " Recall@20: 0.1227 \n",
      " NDCG@20: 0.2623\n",
      "Epoch: 119, Training time: 160.02s, Loss: 50.1586\n",
      "Evaluate current model:\n",
      " Epoch: 119, Validation time: 33.84s \n",
      " Loss: 50.1586: \n",
      " Recall@20: 0.1230 \n",
      " NDCG@20: 0.2620\n",
      "Epoch: 120, Training time: 160.00s, Loss: 49.4312\n",
      "Evaluate current model:\n",
      " Epoch: 120, Validation time: 33.86s \n",
      " Loss: 49.4312: \n",
      " Recall@20: 0.1233 \n",
      " NDCG@20: 0.2625\n",
      "Epoch: 121, Training time: 161.05s, Loss: 49.2836\n",
      "Evaluate current model:\n",
      " Epoch: 121, Validation time: 35.13s \n",
      " Loss: 49.2836: \n",
      " Recall@20: 0.1242 \n",
      " NDCG@20: 0.2629\n",
      "Epoch: 122, Training time: 160.62s, Loss: 48.2467\n",
      "Evaluate current model:\n",
      " Epoch: 122, Validation time: 35.44s \n",
      " Loss: 48.2467: \n",
      " Recall@20: 0.1235 \n",
      " NDCG@20: 0.2630\n",
      "Epoch: 123, Training time: 159.82s, Loss: 47.8290\n",
      "Evaluate current model:\n",
      " Epoch: 123, Validation time: 35.39s \n",
      " Loss: 47.8290: \n",
      " Recall@20: 0.1237 \n",
      " NDCG@20: 0.2636\n",
      "Epoch: 124, Training time: 159.90s, Loss: 47.3706\n",
      "Evaluate current model:\n",
      " Epoch: 124, Validation time: 35.12s \n",
      " Loss: 47.3706: \n",
      " Recall@20: 0.1239 \n",
      " NDCG@20: 0.2632\n",
      "Epoch: 125, Training time: 160.25s, Loss: 46.6939\n",
      "Evaluate current model:\n",
      " Epoch: 125, Validation time: 35.14s \n",
      " Loss: 46.6939: \n",
      " Recall@20: 0.1239 \n",
      " NDCG@20: 0.2635\n",
      "Epoch: 126, Training time: 160.69s, Loss: 46.3724\n",
      "Evaluate current model:\n",
      " Epoch: 126, Validation time: 34.84s \n",
      " Loss: 46.3724: \n",
      " Recall@20: 0.1251 \n",
      " NDCG@20: 0.2651\n",
      "Epoch: 127, Training time: 160.70s, Loss: 45.8238\n",
      "Evaluate current model:\n",
      " Epoch: 127, Validation time: 34.99s \n",
      " Loss: 45.8238: \n",
      " Recall@20: 0.1250 \n",
      " NDCG@20: 0.2653\n",
      "Epoch: 128, Training time: 160.86s, Loss: 45.2476\n",
      "Evaluate current model:\n",
      " Epoch: 128, Validation time: 35.19s \n",
      " Loss: 45.2476: \n",
      " Recall@20: 0.1254 \n",
      " NDCG@20: 0.2647\n",
      "Epoch: 129, Training time: 160.70s, Loss: 44.8177\n",
      "Evaluate current model:\n",
      " Epoch: 129, Validation time: 34.85s \n",
      " Loss: 44.8177: \n",
      " Recall@20: 0.1252 \n",
      " NDCG@20: 0.2661\n",
      "Epoch: 130, Training time: 160.79s, Loss: 44.6539\n",
      "Evaluate current model:\n",
      " Epoch: 130, Validation time: 34.95s \n",
      " Loss: 44.6539: \n",
      " Recall@20: 0.1259 \n",
      " NDCG@20: 0.2660\n",
      "Epoch: 131, Training time: 160.77s, Loss: 44.0121\n",
      "Evaluate current model:\n",
      " Epoch: 131, Validation time: 34.83s \n",
      " Loss: 44.0121: \n",
      " Recall@20: 0.1253 \n",
      " NDCG@20: 0.2652\n",
      "Epoch: 132, Training time: 160.86s, Loss: 43.5387\n",
      "Evaluate current model:\n",
      " Epoch: 132, Validation time: 35.00s \n",
      " Loss: 43.5387: \n",
      " Recall@20: 0.1253 \n",
      " NDCG@20: 0.2659\n",
      "Epoch: 133, Training time: 161.07s, Loss: 43.1109\n",
      "Evaluate current model:\n",
      " Epoch: 133, Validation time: 35.20s \n",
      " Loss: 43.1109: \n",
      " Recall@20: 0.1255 \n",
      " NDCG@20: 0.2661\n",
      "Epoch: 134, Training time: 160.98s, Loss: 43.0100\n",
      "Evaluate current model:\n",
      " Epoch: 134, Validation time: 35.05s \n",
      " Loss: 43.0100: \n",
      " Recall@20: 0.1263 \n",
      " NDCG@20: 0.2666\n",
      "Epoch: 135, Training time: 161.00s, Loss: 42.3687\n",
      "Evaluate current model:\n",
      " Epoch: 135, Validation time: 35.01s \n",
      " Loss: 42.3687: \n",
      " Recall@20: 0.1263 \n",
      " NDCG@20: 0.2671\n",
      "Epoch: 136, Training time: 160.95s, Loss: 42.1183\n",
      "Evaluate current model:\n",
      " Epoch: 136, Validation time: 35.07s \n",
      " Loss: 42.1183: \n",
      " Recall@20: 0.1263 \n",
      " NDCG@20: 0.2677\n",
      "Epoch: 137, Training time: 161.09s, Loss: 41.2776\n",
      "Evaluate current model:\n",
      " Epoch: 137, Validation time: 35.06s \n",
      " Loss: 41.2776: \n",
      " Recall@20: 0.1259 \n",
      " NDCG@20: 0.2680\n",
      "Epoch: 138, Training time: 160.96s, Loss: 41.3429\n",
      "Evaluate current model:\n",
      " Epoch: 138, Validation time: 35.04s \n",
      " Loss: 41.3429: \n",
      " Recall@20: 0.1264 \n",
      " NDCG@20: 0.2679\n",
      "Epoch: 139, Training time: 160.90s, Loss: 40.7146\n",
      "Evaluate current model:\n",
      " Epoch: 139, Validation time: 35.24s \n",
      " Loss: 40.7146: \n",
      " Recall@20: 0.1270 \n",
      " NDCG@20: 0.2680\n",
      "Epoch: 140, Training time: 160.99s, Loss: 40.6610\n",
      "Evaluate current model:\n",
      " Epoch: 140, Validation time: 35.18s \n",
      " Loss: 40.6610: \n",
      " Recall@20: 0.1272 \n",
      " NDCG@20: 0.2689\n",
      "Epoch: 141, Training time: 160.89s, Loss: 40.4660\n",
      "Evaluate current model:\n",
      " Epoch: 141, Validation time: 34.94s \n",
      " Loss: 40.4660: \n",
      " Recall@20: 0.1275 \n",
      " NDCG@20: 0.2691\n",
      "Epoch: 142, Training time: 160.86s, Loss: 39.7284\n",
      "Evaluate current model:\n",
      " Epoch: 142, Validation time: 34.95s \n",
      " Loss: 39.7284: \n",
      " Recall@20: 0.1270 \n",
      " NDCG@20: 0.2691\n",
      "Epoch: 143, Training time: 160.67s, Loss: 39.2928\n",
      "Evaluate current model:\n",
      " Epoch: 143, Validation time: 34.95s \n",
      " Loss: 39.2928: \n",
      " Recall@20: 0.1277 \n",
      " NDCG@20: 0.2696\n",
      "Epoch: 144, Training time: 160.75s, Loss: 39.2214\n",
      "Evaluate current model:\n",
      " Epoch: 144, Validation time: 35.11s \n",
      " Loss: 39.2214: \n",
      " Recall@20: 0.1279 \n",
      " NDCG@20: 0.2688\n",
      "Epoch: 145, Training time: 160.82s, Loss: 38.6664\n",
      "Evaluate current model:\n",
      " Epoch: 145, Validation time: 34.52s \n",
      " Loss: 38.6664: \n",
      " Recall@20: 0.1276 \n",
      " NDCG@20: 0.2695\n",
      "Epoch: 146, Training time: 161.40s, Loss: 38.0121\n",
      "Evaluate current model:\n",
      " Epoch: 146, Validation time: 35.63s \n",
      " Loss: 38.0121: \n",
      " Recall@20: 0.1280 \n",
      " NDCG@20: 0.2696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147, Training time: 161.25s, Loss: 38.2022\n",
      "Evaluate current model:\n",
      " Epoch: 147, Validation time: 32.13s \n",
      " Loss: 38.2022: \n",
      " Recall@20: 0.1281 \n",
      " NDCG@20: 0.2693\n",
      "Epoch: 148, Training time: 158.91s, Loss: 37.6347\n",
      "Evaluate current model:\n",
      " Epoch: 148, Validation time: 32.21s \n",
      " Loss: 37.6347: \n",
      " Recall@20: 0.1279 \n",
      " NDCG@20: 0.2701\n",
      "Epoch: 149, Training time: 158.96s, Loss: 37.3465\n",
      "Evaluate current model:\n",
      " Epoch: 149, Validation time: 32.53s \n",
      " Loss: 37.3465: \n",
      " Recall@20: 0.1286 \n",
      " NDCG@20: 0.2701\n",
      "Epoch: 150, Training time: 158.92s, Loss: 37.2422\n",
      "Evaluate current model:\n",
      " Epoch: 150, Validation time: 32.30s \n",
      " Loss: 37.2422: \n",
      " Recall@20: 0.1286 \n",
      " NDCG@20: 0.2693\n",
      "Epoch: 151, Training time: 158.86s, Loss: 36.6819\n",
      "Evaluate current model:\n",
      " Epoch: 151, Validation time: 32.22s \n",
      " Loss: 36.6819: \n",
      " Recall@20: 0.1288 \n",
      " NDCG@20: 0.2708\n",
      "Epoch: 152, Training time: 158.98s, Loss: 36.5731\n",
      "Evaluate current model:\n",
      " Epoch: 152, Validation time: 32.30s \n",
      " Loss: 36.5731: \n",
      " Recall@20: 0.1292 \n",
      " NDCG@20: 0.2711\n",
      "Epoch: 153, Training time: 158.93s, Loss: 36.3686\n",
      "Evaluate current model:\n",
      " Epoch: 153, Validation time: 32.28s \n",
      " Loss: 36.3686: \n",
      " Recall@20: 0.1294 \n",
      " NDCG@20: 0.2707\n",
      "Epoch: 154, Training time: 158.90s, Loss: 36.0044\n",
      "Evaluate current model:\n",
      " Epoch: 154, Validation time: 32.38s \n",
      " Loss: 36.0044: \n",
      " Recall@20: 0.1293 \n",
      " NDCG@20: 0.2709\n",
      "Epoch: 155, Training time: 159.00s, Loss: 35.7654\n",
      "Evaluate current model:\n",
      " Epoch: 155, Validation time: 32.29s \n",
      " Loss: 35.7654: \n",
      " Recall@20: 0.1295 \n",
      " NDCG@20: 0.2712\n",
      "Epoch: 156, Training time: 158.95s, Loss: 35.4634\n",
      "Evaluate current model:\n",
      " Epoch: 156, Validation time: 32.25s \n",
      " Loss: 35.4634: \n",
      " Recall@20: 0.1297 \n",
      " NDCG@20: 0.2716\n",
      "Epoch: 157, Training time: 158.91s, Loss: 35.2285\n",
      "Evaluate current model:\n",
      " Epoch: 157, Validation time: 32.34s \n",
      " Loss: 35.2285: \n",
      " Recall@20: 0.1297 \n",
      " NDCG@20: 0.2711\n",
      "Epoch: 158, Training time: 158.88s, Loss: 34.9544\n",
      "Evaluate current model:\n",
      " Epoch: 158, Validation time: 32.17s \n",
      " Loss: 34.9544: \n",
      " Recall@20: 0.1293 \n",
      " NDCG@20: 0.2711\n",
      "Epoch: 159, Training time: 158.94s, Loss: 34.8138\n",
      "Evaluate current model:\n",
      " Epoch: 159, Validation time: 32.15s \n",
      " Loss: 34.8138: \n",
      " Recall@20: 0.1299 \n",
      " NDCG@20: 0.2718\n",
      "Epoch: 160, Training time: 158.95s, Loss: 34.5828\n",
      "Evaluate current model:\n",
      " Epoch: 160, Validation time: 32.22s \n",
      " Loss: 34.5828: \n",
      " Recall@20: 0.1304 \n",
      " NDCG@20: 0.2729\n",
      "Epoch: 161, Training time: 159.02s, Loss: 34.2305\n",
      "Evaluate current model:\n",
      " Epoch: 161, Validation time: 32.26s \n",
      " Loss: 34.2305: \n",
      " Recall@20: 0.1303 \n",
      " NDCG@20: 0.2718\n",
      "Epoch: 162, Training time: 158.96s, Loss: 34.0066\n",
      "Evaluate current model:\n",
      " Epoch: 162, Validation time: 32.51s \n",
      " Loss: 34.0066: \n",
      " Recall@20: 0.1299 \n",
      " NDCG@20: 0.2725\n",
      "Epoch: 163, Training time: 158.92s, Loss: 33.7692\n",
      "Evaluate current model:\n",
      " Epoch: 163, Validation time: 32.12s \n",
      " Loss: 33.7692: \n",
      " Recall@20: 0.1297 \n",
      " NDCG@20: 0.2721\n",
      "Epoch: 164, Training time: 158.88s, Loss: 33.6640\n",
      "Evaluate current model:\n",
      " Epoch: 164, Validation time: 32.36s \n",
      " Loss: 33.6640: \n",
      " Recall@20: 0.1308 \n",
      " NDCG@20: 0.2727\n",
      "Epoch: 165, Training time: 158.86s, Loss: 33.3515\n",
      "Evaluate current model:\n",
      " Epoch: 165, Validation time: 32.24s \n",
      " Loss: 33.3515: \n",
      " Recall@20: 0.1307 \n",
      " NDCG@20: 0.2729\n",
      "Epoch: 166, Training time: 158.97s, Loss: 33.3037\n",
      "Evaluate current model:\n",
      " Epoch: 166, Validation time: 32.23s \n",
      " Loss: 33.3037: \n",
      " Recall@20: 0.1305 \n",
      " NDCG@20: 0.2732\n",
      "Epoch: 167, Training time: 158.97s, Loss: 32.9142\n",
      "Evaluate current model:\n",
      " Epoch: 167, Validation time: 32.27s \n",
      " Loss: 32.9142: \n",
      " Recall@20: 0.1311 \n",
      " NDCG@20: 0.2730\n",
      "Epoch: 168, Training time: 158.92s, Loss: 32.6815\n",
      "Evaluate current model:\n",
      " Epoch: 168, Validation time: 32.18s \n",
      " Loss: 32.6815: \n",
      " Recall@20: 0.1312 \n",
      " NDCG@20: 0.2740\n",
      "Epoch: 169, Training time: 158.92s, Loss: 32.5227\n",
      "Evaluate current model:\n",
      " Epoch: 169, Validation time: 32.29s \n",
      " Loss: 32.5227: \n",
      " Recall@20: 0.1321 \n",
      " NDCG@20: 0.2736\n",
      "Epoch: 170, Training time: 158.90s, Loss: 32.2961\n",
      "Evaluate current model:\n",
      " Epoch: 170, Validation time: 32.32s \n",
      " Loss: 32.2961: \n",
      " Recall@20: 0.1312 \n",
      " NDCG@20: 0.2740\n",
      "Epoch: 171, Training time: 159.00s, Loss: 31.9876\n",
      "Evaluate current model:\n",
      " Epoch: 171, Validation time: 32.17s \n",
      " Loss: 31.9876: \n",
      " Recall@20: 0.1312 \n",
      " NDCG@20: 0.2746\n",
      "Epoch: 172, Training time: 158.96s, Loss: 31.5758\n",
      "Evaluate current model:\n",
      " Epoch: 172, Validation time: 32.15s \n",
      " Loss: 31.5758: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2749\n",
      "Epoch: 173, Training time: 159.42s, Loss: 31.6937\n",
      "Evaluate current model:\n",
      " Epoch: 173, Validation time: 32.64s \n",
      " Loss: 31.6937: \n",
      " Recall@20: 0.1320 \n",
      " NDCG@20: 0.2745\n",
      "Epoch: 174, Training time: 158.89s, Loss: 31.1286\n",
      "Evaluate current model:\n",
      " Epoch: 174, Validation time: 32.72s \n",
      " Loss: 31.1286: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2763\n",
      "Epoch: 175, Training time: 159.02s, Loss: 31.2745\n",
      "Evaluate current model:\n",
      " Epoch: 175, Validation time: 32.09s \n",
      " Loss: 31.2745: \n",
      " Recall@20: 0.1316 \n",
      " NDCG@20: 0.2754\n",
      "Epoch: 176, Training time: 158.93s, Loss: 31.2566\n",
      "Evaluate current model:\n",
      " Epoch: 176, Validation time: 32.27s \n",
      " Loss: 31.2566: \n",
      " Recall@20: 0.1320 \n",
      " NDCG@20: 0.2750\n",
      "Epoch: 177, Training time: 158.94s, Loss: 30.5993\n",
      "Evaluate current model:\n",
      " Epoch: 177, Validation time: 32.22s \n",
      " Loss: 30.5993: \n",
      " Recall@20: 0.1320 \n",
      " NDCG@20: 0.2746\n",
      "Epoch: 178, Training time: 158.95s, Loss: 30.4570\n",
      "Evaluate current model:\n",
      " Epoch: 178, Validation time: 32.39s \n",
      " Loss: 30.4570: \n",
      " Recall@20: 0.1314 \n",
      " NDCG@20: 0.2749\n",
      "Epoch: 179, Training time: 158.87s, Loss: 30.0950\n",
      "Evaluate current model:\n",
      " Epoch: 179, Validation time: 32.20s \n",
      " Loss: 30.0950: \n",
      " Recall@20: 0.1321 \n",
      " NDCG@20: 0.2739\n",
      "Epoch: 180, Training time: 158.91s, Loss: 29.8576\n",
      "Evaluate current model:\n",
      " Epoch: 180, Validation time: 32.23s \n",
      " Loss: 29.8576: \n",
      " Recall@20: 0.1320 \n",
      " NDCG@20: 0.2748\n",
      "Epoch: 181, Training time: 159.00s, Loss: 30.1293\n",
      "Evaluate current model:\n",
      " Epoch: 181, Validation time: 32.17s \n",
      " Loss: 30.1293: \n",
      " Recall@20: 0.1324 \n",
      " NDCG@20: 0.2746\n",
      "Epoch: 182, Training time: 159.01s, Loss: 29.9866\n",
      "Evaluate current model:\n",
      " Epoch: 182, Validation time: 32.30s \n",
      " Loss: 29.9866: \n",
      " Recall@20: 0.1325 \n",
      " NDCG@20: 0.2754\n",
      "Epoch: 183, Training time: 158.92s, Loss: 29.7498\n",
      "Evaluate current model:\n",
      " Epoch: 183, Validation time: 32.33s \n",
      " Loss: 29.7498: \n",
      " Recall@20: 0.1327 \n",
      " NDCG@20: 0.2760\n",
      "Epoch: 184, Training time: 158.95s, Loss: 29.8165\n",
      "Evaluate current model:\n",
      " Epoch: 184, Validation time: 32.49s \n",
      " Loss: 29.8165: \n",
      " Recall@20: 0.1330 \n",
      " NDCG@20: 0.2760\n",
      "Epoch: 185, Training time: 158.93s, Loss: 29.4329\n",
      "Evaluate current model:\n",
      " Epoch: 185, Validation time: 32.31s \n",
      " Loss: 29.4329: \n",
      " Recall@20: 0.1330 \n",
      " NDCG@20: 0.2770\n",
      "Epoch: 186, Training time: 158.89s, Loss: 29.4954\n",
      "Evaluate current model:\n",
      " Epoch: 186, Validation time: 32.30s \n",
      " Loss: 29.4954: \n",
      " Recall@20: 0.1327 \n",
      " NDCG@20: 0.2762\n",
      "Epoch: 187, Training time: 158.90s, Loss: 29.0460\n",
      "Evaluate current model:\n",
      " Epoch: 187, Validation time: 32.19s \n",
      " Loss: 29.0460: \n",
      " Recall@20: 0.1327 \n",
      " NDCG@20: 0.2761\n",
      "Epoch: 188, Training time: 158.99s, Loss: 28.7174\n",
      "Evaluate current model:\n",
      " Epoch: 188, Validation time: 32.25s \n",
      " Loss: 28.7174: \n",
      " Recall@20: 0.1333 \n",
      " NDCG@20: 0.2759\n",
      "Epoch: 189, Training time: 158.84s, Loss: 28.5363\n",
      "Evaluate current model:\n",
      " Epoch: 189, Validation time: 32.20s \n",
      " Loss: 28.5363: \n",
      " Recall@20: 0.1337 \n",
      " NDCG@20: 0.2769\n",
      "Epoch: 190, Training time: 158.84s, Loss: 28.4438\n",
      "Evaluate current model:\n",
      " Epoch: 190, Validation time: 32.30s \n",
      " Loss: 28.4438: \n",
      " Recall@20: 0.1333 \n",
      " NDCG@20: 0.2769\n",
      "Epoch: 191, Training time: 158.97s, Loss: 28.4365\n",
      "Evaluate current model:\n",
      " Epoch: 191, Validation time: 32.23s \n",
      " Loss: 28.4365: \n",
      " Recall@20: 0.1334 \n",
      " NDCG@20: 0.2770\n",
      "Epoch: 192, Training time: 158.96s, Loss: 28.1797\n",
      "Evaluate current model:\n",
      " Epoch: 192, Validation time: 32.33s \n",
      " Loss: 28.1797: \n",
      " Recall@20: 0.1333 \n",
      " NDCG@20: 0.2761\n",
      "Epoch: 193, Training time: 158.92s, Loss: 28.3155\n",
      "Evaluate current model:\n",
      " Epoch: 193, Validation time: 32.46s \n",
      " Loss: 28.3155: \n",
      " Recall@20: 0.1332 \n",
      " NDCG@20: 0.2761\n",
      "Epoch: 194, Training time: 158.93s, Loss: 27.8054\n",
      "Evaluate current model:\n",
      " Epoch: 194, Validation time: 32.36s \n",
      " Loss: 27.8054: \n",
      " Recall@20: 0.1340 \n",
      " NDCG@20: 0.2767\n",
      "Epoch: 195, Training time: 158.89s, Loss: 27.6005\n",
      "Evaluate current model:\n",
      " Epoch: 195, Validation time: 32.41s \n",
      " Loss: 27.6005: \n",
      " Recall@20: 0.1340 \n",
      " NDCG@20: 0.2775\n",
      "Epoch: 196, Training time: 158.89s, Loss: 27.6820\n",
      "Evaluate current model:\n",
      " Epoch: 196, Validation time: 32.38s \n",
      " Loss: 27.6820: \n",
      " Recall@20: 0.1336 \n",
      " NDCG@20: 0.2772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197, Training time: 159.05s, Loss: 27.3788\n",
      "Evaluate current model:\n",
      " Epoch: 197, Validation time: 32.29s \n",
      " Loss: 27.3788: \n",
      " Recall@20: 0.1337 \n",
      " NDCG@20: 0.2766\n",
      "Epoch: 198, Training time: 158.93s, Loss: 27.2159\n",
      "Evaluate current model:\n",
      " Epoch: 198, Validation time: 32.22s \n",
      " Loss: 27.2159: \n",
      " Recall@20: 0.1343 \n",
      " NDCG@20: 0.2773\n",
      "Epoch: 199, Training time: 158.95s, Loss: 27.4823\n",
      "Evaluate current model:\n",
      " Epoch: 199, Validation time: 32.21s \n",
      " Loss: 27.4823: \n",
      " Recall@20: 0.1342 \n",
      " NDCG@20: 0.2784\n",
      "Epoch: 200, Training time: 158.94s, Loss: 27.1938\n",
      "Evaluate current model:\n",
      " Epoch: 200, Validation time: 32.17s \n",
      " Loss: 27.1938: \n",
      " Recall@20: 0.1338 \n",
      " NDCG@20: 0.2777\n",
      "Epoch: 201, Training time: 159.01s, Loss: 26.7685\n",
      "Evaluate current model:\n",
      " Epoch: 201, Validation time: 32.34s \n",
      " Loss: 26.7685: \n",
      " Recall@20: 0.1344 \n",
      " NDCG@20: 0.2780\n",
      "Epoch: 202, Training time: 158.90s, Loss: 26.4051\n",
      "Evaluate current model:\n",
      " Epoch: 202, Validation time: 32.25s \n",
      " Loss: 26.4051: \n",
      " Recall@20: 0.1349 \n",
      " NDCG@20: 0.2775\n",
      "Epoch: 203, Training time: 159.03s, Loss: 26.6173\n",
      "Evaluate current model:\n",
      " Epoch: 203, Validation time: 32.23s \n",
      " Loss: 26.6173: \n",
      " Recall@20: 0.1346 \n",
      " NDCG@20: 0.2777\n",
      "Epoch: 204, Training time: 159.01s, Loss: 26.4195\n",
      "Evaluate current model:\n",
      " Epoch: 204, Validation time: 32.17s \n",
      " Loss: 26.4195: \n",
      " Recall@20: 0.1343 \n",
      " NDCG@20: 0.2768\n",
      "Epoch: 205, Training time: 158.89s, Loss: 26.2722\n",
      "Evaluate current model:\n",
      " Epoch: 205, Validation time: 32.15s \n",
      " Loss: 26.2722: \n",
      " Recall@20: 0.1348 \n",
      " NDCG@20: 0.2769\n",
      "Epoch: 206, Training time: 158.95s, Loss: 26.1019\n",
      "Evaluate current model:\n",
      " Epoch: 206, Validation time: 32.39s \n",
      " Loss: 26.1019: \n",
      " Recall@20: 0.1351 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 207, Training time: 158.97s, Loss: 25.8995\n",
      "Evaluate current model:\n",
      " Epoch: 207, Validation time: 32.19s \n",
      " Loss: 25.8995: \n",
      " Recall@20: 0.1343 \n",
      " NDCG@20: 0.2769\n",
      "Epoch: 208, Training time: 158.96s, Loss: 26.0937\n",
      "Evaluate current model:\n",
      " Epoch: 208, Validation time: 32.06s \n",
      " Loss: 26.0937: \n",
      " Recall@20: 0.1345 \n",
      " NDCG@20: 0.2776\n",
      "Epoch: 209, Training time: 158.96s, Loss: 25.9144\n",
      "Evaluate current model:\n",
      " Epoch: 209, Validation time: 32.31s \n",
      " Loss: 25.9144: \n",
      " Recall@20: 0.1344 \n",
      " NDCG@20: 0.2778\n",
      "Epoch: 210, Training time: 158.91s, Loss: 25.5979\n",
      "Evaluate current model:\n",
      " Epoch: 210, Validation time: 32.25s \n",
      " Loss: 25.5979: \n",
      " Recall@20: 0.1352 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 211, Training time: 159.22s, Loss: 25.5673\n",
      "Evaluate current model:\n",
      " Epoch: 211, Validation time: 32.29s \n",
      " Loss: 25.5673: \n",
      " Recall@20: 0.1351 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 212, Training time: 158.97s, Loss: 25.5335\n",
      "Evaluate current model:\n",
      " Epoch: 212, Validation time: 32.26s \n",
      " Loss: 25.5335: \n",
      " Recall@20: 0.1353 \n",
      " NDCG@20: 0.2777\n",
      "Epoch: 213, Training time: 158.98s, Loss: 25.3916\n",
      "Evaluate current model:\n",
      " Epoch: 213, Validation time: 32.25s \n",
      " Loss: 25.3916: \n",
      " Recall@20: 0.1353 \n",
      " NDCG@20: 0.2785\n",
      "Epoch: 214, Training time: 158.96s, Loss: 25.2609\n",
      "Evaluate current model:\n",
      " Epoch: 214, Validation time: 32.99s \n",
      " Loss: 25.2609: \n",
      " Recall@20: 0.1355 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 215, Training time: 158.90s, Loss: 24.9891\n",
      "Evaluate current model:\n",
      " Epoch: 215, Validation time: 32.64s \n",
      " Loss: 24.9891: \n",
      " Recall@20: 0.1351 \n",
      " NDCG@20: 0.2782\n",
      "Epoch: 216, Training time: 158.84s, Loss: 24.7840\n",
      "Evaluate current model:\n",
      " Epoch: 216, Validation time: 32.36s \n",
      " Loss: 24.7840: \n",
      " Recall@20: 0.1357 \n",
      " NDCG@20: 0.2791\n",
      "Epoch: 217, Training time: 158.34s, Loss: 24.5573\n",
      "Evaluate current model:\n",
      " Epoch: 217, Validation time: 32.03s \n",
      " Loss: 24.5573: \n",
      " Recall@20: 0.1359 \n",
      " NDCG@20: 0.2787\n",
      "Epoch: 218, Training time: 158.60s, Loss: 24.6107\n",
      "Evaluate current model:\n",
      " Epoch: 218, Validation time: 33.75s \n",
      " Loss: 24.6107: \n",
      " Recall@20: 0.1356 \n",
      " NDCG@20: 0.2789\n",
      "Epoch: 219, Training time: 158.48s, Loss: 24.7029\n",
      "Evaluate current model:\n",
      " Epoch: 219, Validation time: 32.39s \n",
      " Loss: 24.7029: \n",
      " Recall@20: 0.1359 \n",
      " NDCG@20: 0.2791\n",
      "Epoch: 220, Training time: 158.98s, Loss: 24.5867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     66\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 67\u001b[0m     recall, ndcg \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_g_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi_g_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate current model:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Validation time: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, time()\u001b[38;5;241m-\u001b[39mt2),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, ndcg)\n\u001b[0;32m     78\u001b[0m     )\n\u001b[0;32m     80\u001b[0m cur_best_metric, stopping_step, should_stop \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     81\u001b[0m early_stopping(recall, cur_best_metric, stopping_step, flag_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(u_emb, i_emb, Rtr, Rte, k)\u001b[0m\n\u001b[0;32m    113\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(ue_f, i_emb\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m    115\u001b[0m test_items \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(te_f\u001b[38;5;241m.\u001b[39mtodense())\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m--> 116\u001b[0m non_train_items \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m(\u001b[43mtr_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtodense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    117\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m*\u001b[39m non_train_items\n\u001b[0;32m    119\u001b[0m _, test_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, k\u001b[38;5;241m=\u001b[39mk)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\scipy\\sparse\\_base.py:939\u001b[0m, in \u001b[0;36mspmatrix.todense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtodense\u001b[39m(\u001b[38;5;28mself\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Return a dense matrix representation of this matrix.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m        `numpy.matrix` object that shares the same memory.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascontainer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\scipy\\sparse\\_base.py:971\u001b[0m, in \u001b[0;36mspmatrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    Return a dense ndarray representation of this matrix.\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;124;03m        appropriate values.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray(order\u001b[38;5;241m=\u001b[39morder, out\u001b[38;5;241m=\u001b[39mout)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\scipy\\sparse\\_dok.py:398\u001b[0m, in \u001b[0;36mdok_matrix.tocoo\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    396\u001b[0m row \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter((i \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()), dtype\u001b[38;5;241m=\u001b[39midx_dtype, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz)\n\u001b[0;32m    397\u001b[0m col \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter((j \u001b[38;5;28;01mfor\u001b[39;00m _, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()), dtype\u001b[38;5;241m=\u001b[39midx_dtype, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz)\n\u001b[1;32m--> 398\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coo_container\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m A\u001b[38;5;241m.\u001b[39mhas_canonical_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\scipy\\sparse\\_coo.py:196\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\scipy\\sparse\\_coo.py:286\u001b[0m, in \u001b[0;36mcoo_matrix._check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn index exceeds matrix dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative row index found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ngcfpytorch\\lib\\site-packages\\numpy\\core\\_methods.py:44\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from ngcf import NGCF\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "\n",
    "# generate the NGCF-adjacency matrix\n",
    "data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\n",
    "adj_mtx = data_generator.get_adj_mat()\n",
    "\n",
    "# create model name and save\n",
    "modelname =  \"NGCF\" + \\\n",
    "    \"_bs_\" + str(batch_size) + \\\n",
    "    \"_nemb_\" + str(emb_dim) + \\\n",
    "    \"_layers_\" + str(layers) + \\\n",
    "    \"_nodedr_\" + str(node_dropout) + \\\n",
    "    \"_messdr_\" + str(mess_dropout) + \\\n",
    "    \"_reg_\" + str(reg) + \\\n",
    "    \"_lr_\"  + str(lr)\n",
    "\n",
    "# create NGCF model\n",
    "model = NGCF(data_generator.n_users, \n",
    "              data_generator.n_items,\n",
    "              emb_dim,\n",
    "              layers,\n",
    "              reg,\n",
    "              node_dropout,\n",
    "              mess_dropout,\n",
    "              adj_mtx)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# current best metric\n",
    "cur_best_metric = 0\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Set values for early stopping\n",
    "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
    "today = datetime.now()\n",
    "\n",
    "print(\"Start at \" + str(today))\n",
    "print(\"Using \" + str(device) + \" for computations\")\n",
    "print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\n",
    "\n",
    "results = {\"Epoch\": [],\n",
    "            \"Loss\": [],\n",
    "            \"Recall\": [],\n",
    "            \"NDCG\": [],\n",
    "            \"Training Time\": []}\n",
    "\n",
    "for epoch in range(argsn_epochs):\n",
    "\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    training_time = time()-t1\n",
    "    print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\n",
    "        format(epoch, training_time, loss))\n",
    "\n",
    "    # print test evaluation metrics every N epochs (provided by args.eval_N)\n",
    "    if epoch % argseval_N  == (argseval_N - 1):\n",
    "        with torch.no_grad():\n",
    "            t2 = time()\n",
    "            recall, ndcg = eval_model(model.u_g_embeddings.detach(),\n",
    "                                      model.i_g_embeddings.detach(),\n",
    "                                      data_generator.R_train,\n",
    "                                      data_generator.R_test,\n",
    "                                      k)\n",
    "        print(\n",
    "            \"Evaluate current model:\\n\",\n",
    "            \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Loss: {:.4f}:\".format(loss), \"\\n\",\n",
    "            \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\n",
    "            \"NDCG@{}: {:.4f}\".format(k, ndcg)\n",
    "            )\n",
    "\n",
    "        cur_best_metric, stopping_step, should_stop = \\\n",
    "        early_stopping(recall, cur_best_metric, stopping_step, flag_step=50)\n",
    "\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(recall.item())\n",
    "        results['NDCG'].append(ndcg.item())\n",
    "        results['Training Time'].append(training_time)\n",
    "    else:\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(None)\n",
    "        results['NDCG'].append(None)\n",
    "        results['Training Time'].append(training_time)\n",
    "\n",
    "    if should_stop == True: break\n",
    "\n",
    "# save\n",
    "if argssave_results:\n",
    "    date = today.strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "    # save model as .pt file\n",
    "    if os.path.isdir(\"./models\"):\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "    else:\n",
    "        os.mkdir(\"./models\")\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "\n",
    "    # save results as pandas dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.set_index('Epoch', inplace=True)\n",
    "    if os.path.isdir(\"./results\"):\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    else:\n",
    "        os.mkdir(\"./results\")\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    # plot loss\n",
    "    results_df['Loss'].plot(figsize=(12,8), title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPFD82S7p24NDxgz8stBRuM",
   "name": "NGCFPytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
